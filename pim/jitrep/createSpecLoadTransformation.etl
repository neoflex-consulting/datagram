import "jrutils.etl";
pre {
    var dataset = S!jitdataset::Dataset.all.first();
    var deployment = S!jitdataset::DeploymentDescriptor.all.first();
    var trName = prefix + dataset.name.toLowerCase();
    trName.println("Create transformation: ");
    deployment.name.println("DeploymentDescriptor: ");
}
post {
    "Done".println;
}

@greedy
rule Dataset2TransformationBuild
    transform dataset: S!Dataset
    to transformation: T!Transformation {
    guard: dataset.buildSpec.isDefined()
    "Dataset2Transformation".println();
    transformation.name = trName;
    transformation.project = dataset.project;
    transformation.addParameter("task_id", "TID${(System.currentTimeMillis/1000).toInt}", false);
    for (field in dataset.paramFields()) {
        transformation.addParameter(field.name.toLowerCase(), "null", true);
    }
    var buildSpec = dataset.buildSpec;
    if (not buildSpec.isKindOf(S!jitdataset::SQLBuildSpec)) {
        throw "only SQLBuildSpec is supported";
    }
// SparkSQL
    var sqlStep = transformation.createStep(new T!etl::SparkSQL, "SparkSQL");
    transformation.createOutputPort(sqlStep);
    for (field in dataset.specFields()) {
        sqlStep.outputPort.addField(new T!dataset::Field, field.name.toLowerCase(), field.dataType.getDomain());        
    }
    sqlStep.statement = dataset.interpolareParameters(buildSpec.sql);
    sqlStep.checkpoint = true;
// HiveSource-s
    for (usedDataset in buildSpec.usedDatasets) {
        var source = null;
        if (usedDataset.dataset.isKindOf(S!jitdataset::ExternalDataset)) {
            source = transformation.createSource(new T!etl::SQLSource, usedDataset.dataset.name + "Source");
            source.statement = usedDataset.dataset.sqlExpr; 
            source.context = usedDataset.dataset.context;
            transformation.createOutputPort(source);
            for (field in usedDataset.dataset.allFields()) {
                source.outputPort.addField(new T!dataset::Field, field.name, field.dataType.getDomain());        
            }
        }
        else {
	        source = transformation.createSource(new T!etl::HiveSource, usedDataset.dataset.name + "Source");
	        source.statement = dataset.interpolareParameters(usedDataset.createSQL());
            transformation.createOutputPort(source);
            for (field in usedDataset.dataset.allFields()) {
                source.outputPort.addField(new T!dataset::Field, field.name.toLowerCase(), field.dataType.getDomain());        
            }
        }
        var inputPort = new T!etl::SQLPort;
        inputPort.name = source.name + "_Input";
        inputPort.`alias` = usedDataset.`alias`;
        transformation.link(source.outputPort, inputPort);
        sqlStep.sqlPorts.add(inputPort);
    }
// Selection
    var selectionStep = transformation.createStep(new T!etl::Selection, "Selection");
    transformation.createInputPort(selectionStep, sqlStep.outputPort);
    transformation.createOutputPort(selectionStep).copyFields(selectionStep.inputPort);
    selectionStep.expression = "true";
    if (dataset.hasParameters()) {
        selectionStep.expression = "";
        for (field in dataset.paramFields()) {
            selectionStep.expression = selectionStep.expression + '(jobParameters("' + field.name.toLowerCase() + '") == null || ' + field.name.toLowerCase() + ' != null && '  + field.name.toLowerCase() +  '.equals(' + parameterToScalaDomain(field) + '))';
            if (hasMore) {
                selectionStep.expression = selectionStep.expression + ' && ';
            }
        }
    }
    selectionStep.checkpoint = true;
// FullProjection
    var projectionStep = transformation.createStep(new T!etl::Projection, "FullProjection");
    transformation.createInputPort(projectionStep, selectionStep.outputPort);
    transformation.createOutputPort(projectionStep);
    for (field in dataset.allFields()) {
        var pfield = new T!etl::ProjectionField;
        var ifield = projectionStep.inputPort.findField(field.name.toLowerCase()); 
        if (ifield.isDefined()) {
            pfield.fieldOperationType = T!etl::FieldOperationType#ADD;
            pfield.expression = ifield.name;
            pfield.sourceFields.add(ifield);
        }
        else {
            pfield.fieldOperationType = T!etl::FieldOperationType#TRANSFORM;
            pfield.expression = "null";
        }
        projectionStep.outputPort.addField(pfield, field.name.toLowerCase(), field.dataType.getDomain());
    }
// ExpressionSource
    var expSource = transformation.createSource(new T!etl::ExpressionSource, "ExpressionSource");
    transformation.createOutputPort(expSource);
    expSource.outputPort.addField(new T!dataset::Field, "task_id", T!DataTypeDomain#STRING);
    expSource.outputPort.addField(new T!dataset::Field, "data_version_id", T!DataTypeDomain#INTEGER);
    expSource.outputPort.addField(new T!dataset::Field, "src_data_version_id", T!DataTypeDomain#INTEGER);
    expSource.outputPort.addField(new T!dataset::Field, "load_from_csv", T!DataTypeDomain#BOOLEAN);
    expSource.outputPort.addField(new T!dataset::Field, "load_by_spec", T!DataTypeDomain#BOOLEAN);
    expSource.outputPort.addField(new T!dataset::Field, "classify", T!DataTypeDomain#BOOLEAN);
    expSource.outputPort.name = expSource.name + "_Output";
    expSource.expression = 'Array[Map[String,AnyRef]](\nMap(' +
    '"task_id" -> jobParameters("task_id").asInstanceOf[String],\n' + 
    '"data_version_id" -> INTEGER((System.currentTimeMillis/1000).toInt),\n' + 
    '"src_data_version_id" -> null,\n' +  
    '"load_from_csv" -> Boolean.box(false),\n' + 
    '"load_by_spec" -> Boolean.box(true),\n' + 
    '"classify" -> Boolean.box(false)'; 
    for (field in dataset.paramFields()) {
        expSource.expression = expSource.expression + ',\n"' + '_p_' + field.name.toLowerCase() + '" -> ' + parameterToScalaDomain(field);
        expSource.outputPort.addField(new T!dataset::Field, '_p_' + field.name.toLowerCase(), field.dataType.getDomain());
    }   
    expSource.expression = expSource.expression + '\n))';
    var lastStep = expSource;
    for (usedDataset in buildSpec.usedDatasets.select(ud|ud.dataset.isKindOf(S!Dataset))) {
// ResSource
        var source = transformation.createSource(new T!etl::HiveSource, usedDataset.dataset.name + "ResSource");
        transformation.createOutputPort(source);
        source.outputPort.addField(new T!dataset::Field, "max_data_version_id", T!DataTypeDomain#INTEGER);
        source.statement = "SELECT MAX(data_version_id) as max_data_version_id\nFROM " + usedDataset.dataset.name.toLowerCase() + "_res";
        if (usedDataset.parameterConstraints.size() > 0) {
            source.statement = source.statement + "\n" + usedDataset.createWhere();
        }
        source.statement = dataset.interpolareParameters(source.statement);
// MaxJoin
        var joinStep = transformation.createStep(new T!etl::Join, "MaxJoin");
        transformation.createInputPort(joinStep, lastStep.outputPort);
        joinStep.joineePort = new T!etl::Port;
        joinStep.joineePort.name = joinStep.name + "_Join";
        transformation.link(source.outputPort, joinStep.joineePort);
        transformation.createOutputPort(joinStep);
        joinStep.outputPort.copyProjectionFields(joinStep.inputPort);
        var pfield = joinStep.outputPort.findField("src_data_version_id"); 
        pfield.sourceFields.add(joinStep.joineePort.findField("max_data_version_id"));
        pfield.fieldOperationType = T!etl::FieldOperationType#TRANSFORM;
        pfield.expression = "if (src_data_version_id == null) {max_data_version_id} else {if (src_data_version_id.compareTo(max_data_version_id) > 0) {src_data_version_id} else {max_data_version_id}}";
    
        lastStep = joinStep;
    }
// ResultAggregation
    var aggregationStep = transformation.createStep(new T!etl::Aggregation, "ResultAggregation");
    transformation.createInputPort(aggregationStep, selectionStep.outputPort);
    transformation.createOutputPort(aggregationStep).copyFields(aggregationStep.inputPort);
    aggregationStep.aggregationFunction = T!etl::AggregationFunction#COUNT;
    aggregationStep.resultFieldName = "_c_" + sqlStep.outputPort.fields[0].name;
    aggregationStep.outputPort.addField(new T!dataset::Field, aggregationStep.resultFieldName, T!DataTypeDomain#DECIMAL);
    aggregationStep.fieldName = sqlStep.outputPort.fields[0].name;
    for (field in dataset.paramFields()) {
        aggregationStep.groupByFieldName.add(field.name.toLowerCase());
    }
// ResultJoin
    var resJoinStep = transformation.createStep(new T!etl::Join, "ResultJoin");
    transformation.createInputPort(resJoinStep, aggregationStep.outputPort);
    resJoinStep.joineePort = new T!etl::Port;
    resJoinStep.joineePort.name = resJoinStep.name + "_Join";
    transformation.link(lastStep.outputPort, resJoinStep.joineePort);
    transformation.createOutputPort(resJoinStep);
    resJoinStep.outputPort.copyProjectionFieldsFiltered(resJoinStep.joineePort, Sequence{'task_id', 'data_version_id', 'src_data_version_id', 'load_from_csv', 'load_by_spec', 'classify'});
    resJoinStep.outputPort.copyProjectionFieldsFiltered(resJoinStep.inputPort, dataset.paramFields().collect(f|f.name.toLowerCase()));
    for (field in dataset.paramFields()) {
        var fieldName = field.name.toLowerCase();
        fieldName.println();
        var ofield = resJoinStep.outputPort.findField(fieldName); 
        var jfield = resJoinStep.joineePort.findField("_p_" + fieldName);
        resJoinStep.joineePort.fields.collect(f|f.name).println(); 
        ofield.sourceFields.add(jfield);
        ofield.fieldOperationType = T!etl::FieldOperationType#TRANSFORM;
        ofield.expression = field.dataType.asScalaType("NVL(" + fieldName + ", _p_" + fieldName + ")");
    }
    resJoinStep.joinType = T!etl::JoinType#FULL;
// ResultSelection
    var resSelectionStep = transformation.createStep(new T!etl::Selection, "ResultSelection");
    transformation.createInputPort(resSelectionStep, resJoinStep.outputPort);
    transformation.createOutputPort(resSelectionStep).copyFields(resSelectionStep.inputPort);
    resSelectionStep.expression = "true";
    if (dataset.hasParameters()) {
        resSelectionStep.expression = "";
        for (field in dataset.paramFields()) {
            resSelectionStep.expression = resSelectionStep.expression + 'DEFINED(' + field.name.toLowerCase() + ')';
            if (hasMore) {
                resSelectionStep.expression = resSelectionStep.expression + ' && ';
            }
        }
    }
    resSelectionStep.checkpoint = true;
// DeleteLoadTarget
    var loadTarget = transformation.createTarget(new T!etl::LocalTarget, "DeleteLoadTarget");
    transformation.createInputPort(loadTarget, resSelectionStep.outputPort);
    loadTarget.localFileName = deployment.hdfsBaseDirectory + "/" + deployment.hdfsUser + "/datasets/" + dataset.name.toLowerCase() + "_ld";
    loadTarget.localFileFormat = T!etl::LocalFileFormat#PARQUET;
    loadTarget.deleteBeforeSave = true; 
    loadTarget.saveMode = T!etl::SaveMode#DISCARD;
    loadTarget.partitions = dataset.paramFields().collect(f|f.name.toLowerCase());
    loadTarget.createFieldMappings();
// DeleteFullTarget
    var fullTarget = transformation.createTarget(new T!etl::LocalTarget, "DeleteFullTarget");
    transformation.createInputPort(fullTarget, resSelectionStep.outputPort);
    fullTarget.localFileName = deployment.hdfsBaseDirectory + "/" + deployment.hdfsUser + "/datasets/" + dataset.name.toLowerCase() + "";
    fullTarget.localFileFormat = T!etl::LocalFileFormat#PARQUET;
    fullTarget.deleteBeforeSave = true; 
    fullTarget.saveMode = T!etl::SaveMode#DISCARD;
    fullTarget.partitions = dataset.paramFields().collect(f|f.name.toLowerCase());
    fullTarget.createFieldMappings();
// LoadTarget
    var buildTarget = transformation.createTarget(new T!etl::LocalTarget, "LoadTarget");
    transformation.createInputPort(buildTarget, selectionStep.outputPort);
    buildTarget.localFileName = deployment.hdfsBaseDirectory + "/" + deployment.hdfsUser + "/datasets/" + dataset.name.toLowerCase() + "_ld";
    buildTarget.localFileFormat = T!etl::LocalFileFormat#PARQUET;
    buildTarget.deleteBeforeSave = true; 
    buildTarget.saveMode = T!etl::SaveMode#APPEND;
    buildTarget.partitions = dataset.paramFields().collect(f|f.name.toLowerCase());
    buildTarget.createFieldMappings();
// FullTarget
    var fullTarget = transformation.createTarget(new T!etl::LocalTarget, "FullTarget");
    transformation.createInputPort(fullTarget, projectionStep.outputPort);
    fullTarget.localFileName = deployment.hdfsBaseDirectory + "/" + deployment.hdfsUser + "/datasets/" + dataset.name.toLowerCase() + "";
    fullTarget.localFileFormat = T!etl::LocalFileFormat#PARQUET;
    fullTarget.deleteBeforeSave = true; 
    fullTarget.saveMode = T!etl::SaveMode#APPEND;
    fullTarget.partitions = dataset.paramFields().collect(f|f.name.toLowerCase());
    fullTarget.createFieldMappings();
// ResultTarget    
    var resultTarget = transformation.createTarget(new T!etl::LocalTarget, "ResultTarget");
    transformation.createInputPort(resultTarget, resSelectionStep.outputPort);
    resultTarget.localFileName = deployment.hdfsBaseDirectory + "/" + deployment.hdfsUser + "/datasets/" + dataset.name.toLowerCase() + "_res";
    resultTarget.localFileFormat = T!etl::LocalFileFormat#PARQUET;
    resultTarget.saveMode = T!etl::SaveMode#APPEND;
    resultTarget.deleteBeforeSave = true; 
    resultTarget.partitions = dataset.paramFields().collect(f|f.name.toLowerCase());
    resultTarget.createFieldMappings();
}

@greedy
rule Dataset2TransformationDeployment
    transform dataset: S!Dataset
    to transformationDeployment: T!TransformationDeployment {
    transformationDeployment.name = "autogenerated_tr_" + trName;
    transformationDeployment.project = dataset.project;
    var transformation: T!Transformation ::= dataset;
    transformationDeployment.transformation = transformation;
    transformationDeployment.jobServer = deployment.sparkJobServer;
    transformationDeployment.debug = false; 
    transformationDeployment.slideSize = 400;
    transformationDeployment.rejectSize = 1000;
    transformationDeployment.fetchSize = 1000;
    transformationDeployment.partitionNum = 1;
    transformationDeployment.persistOnDisk = true;
    transformationDeployment.deployments.addAll(deployment.deployments);
}
