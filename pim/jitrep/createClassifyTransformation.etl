import "jrutils.etl";
pre {
    var dataset = S!jitdataset::Dataset.all.first();
    var deployment = S!jitdataset::DeploymentDescriptor.all.first();
    var trName = prefix + dataset.name.toLowerCase();
    trName.println("Create transformation: ");
    deployment.name.println("DeploymentDescriptor: ");
}
post {
    "Done".println;
}

@greedy
rule Dataset2TransformationBuild
    transform dataset: S!Dataset
    to transformation: T!Transformation {
    "Dataset2Transformation".println();
    transformation.name = trName;
    transformation.project = dataset.project;
    transformation.addParameter("task_id", "TID${(System.currentTimeMillis/1000).toInt}", false);
    for (field in dataset.paramFields()) {
        transformation.addParameter(field.name.toLowerCase(), "null", true);
    }

// HiveSource
	var source = transformation.createSource(new T!etl::HiveSource, dataset.name + "HiveSource");
	transformation.createOutputPort(source);
	for (field in dataset.allFields()) {
	    source.outputPort.addField(new T!dataset::Field, field.name.toLowerCase(), field.dataType.getDomain());
	    if (field.isKindOf(ClassifiedField)) {
            source.outputPort.addField(new T!dataset::Field, field.name.toLowerCase() + "_rule", T!dataset::DataTypeDomain#STRING);
	    }        
	}
	source.statement = dataset.createSQLFromLoad();
// Drools
    var droolsStep = transformation.createStep(new T!etl::Drools, "Drools");
    transformation.createInputPort(droolsStep, source.outputPort);
    transformation.createOutputPort(droolsStep).copyFields(droolsStep.inputPort);
    var rulesFile = new T!etl::DroolsRulesFile;
    rulesFile.fileUrl = deployment.hdfsBaseDirectory + "/" + deployment.hdfsUser + "/rules/" + dataset.name.toLowerCase() + ".drl";
    rulesFile.fileType = T!etl::DroolsFileType#DRL;
    rulesFile.hdfs = true;
    droolsStep.rulesFiles.add(rulesFile);
    droolsStep.inputFactTypeName = "ru.neoflex.meta.rules." + dataset.name;
    droolsStep.resultFactTypeName = "ru.neoflex.meta.rules." + dataset.name;
    droolsStep.resultQueryName = "ResultQuery";
    droolsStep.resultFactName = "$fact";
// FullTarget
    var fullTarget = transformation.createTarget(new T!etl::LocalTarget, "FullTarget");
    transformation.createInputPort(fullTarget, droolsStep.outputPort);
    fullTarget.localFileName = deployment.hdfsBaseDirectory + "/" + deployment.hdfsUser + "/datasets/" + dataset.name.toLowerCase() + "";
    fullTarget.localFileFormat = T!etl::LocalFileFormat#PARQUET;
    fullTarget.deleteBeforeSave = true; 
    fullTarget.saveMode = T!etl::SaveMode#APPEND;
    fullTarget.partitions = dataset.paramFields().collect(f|f.name.toLowerCase());
    fullTarget.createFieldMappings();
// ExpressionSource
    var expSource = transformation.createSource(new T!etl::ExpressionSource, "ExpressionSource");
    transformation.createOutputPort(expSource);
    expSource.outputPort.addField(new T!dataset::Field, "task_id", T!DataTypeDomain#STRING);
    expSource.outputPort.addField(new T!dataset::Field, "data_version_id", T!DataTypeDomain#INTEGER);
    expSource.outputPort.addField(new T!dataset::Field, "src_data_version_id", T!DataTypeDomain#INTEGER);
    expSource.outputPort.addField(new T!dataset::Field, "load_from_csv", T!DataTypeDomain#BOOLEAN);
    expSource.outputPort.addField(new T!dataset::Field, "load_by_spec", T!DataTypeDomain#BOOLEAN);
    expSource.outputPort.addField(new T!dataset::Field, "classify", T!DataTypeDomain#BOOLEAN);
    expSource.outputPort.name = expSource.name + "_Output";
    expSource.expression = 'Array[Map[String,AnyRef]](\nMap(' +
    '"task_id" -> jobParameters("task_id").asInstanceOf[String],\n' + 
    '"data_version_id" -> INTEGER((System.currentTimeMillis/1000).toInt),\n' + 
    '"src_data_version_id" -> null,\n' +  
    '"load_from_csv" -> Boolean.box(false),\n' + 
    '"load_by_spec" -> Boolean.box(false),\n' + 
    '"classify" -> Boolean.box(true)'; 
    expSource.expression = expSource.expression + '\n))';
// HiveResultSource
    var resultSource = transformation.createSource(new T!etl::HiveSource, dataset.name + "HiveResultSource");
    transformation.createOutputPort(resultSource);
    resultSource.outputPort.addField(new T!dataset::Field, "max_src_data_version_id", T!DataTypeDomain#INTEGER);
    resultSource.outputPort.name = resultSource.name + "_Output";
    resultSource.statement = "SELECT MAX(src_data_version_id) AS max_src_data_version_id";
    if (dataset.hasParameters()) {
        for (field in dataset.paramFields()) {
            resultSource.statement = resultSource.statement  + ",\n  " + field.name.toLowerCase();
            resultSource.outputPort.addField(new T!dataset::Field, field.name.toLowerCase(), field.dataType.getDomain());
        }
    }
    resultSource.statement = resultSource.statement  + "\nFROM  " + dataset.name.toLowerCase() + "_res";
    resultSource.statement = resultSource.statement  + dataset.createWhereWithParams();
    if (dataset.hasParameters()) {
        resultSource.statement = resultSource.statement + "\nGROUP BY ";
        for (field in dataset.paramFields()) {
            resultSource.statement = resultSource.statement  + field.name.toLowerCase();
            if (hasMore) {
                resultSource.statement = resultSource.statement  + ", ";
            }
        }
    }
// ResultJoin
    var resJoinStep = transformation.createStep(new T!etl::Join, "ResultJoin");
    transformation.createInputPort(resJoinStep, expSource.outputPort);
    resJoinStep.joineePort = new T!etl::Port;
    resJoinStep.joineePort.name = resJoinStep.name + "_Join";
    transformation.link(resultSource.outputPort, resJoinStep.joineePort);
    transformation.createOutputPort(resJoinStep);
    resJoinStep.outputPort.copyProjectionFields(resJoinStep.inputPort);
    resJoinStep.outputPort.copyProjectionFieldsFiltered(resJoinStep.joineePort, dataset.paramFields().collect(f|f.name.toLowerCase()));
    var maxfield = resJoinStep.joineePort.fields[0];
    var outfield = resJoinStep.outputPort.findField('src_data_version_id');
    outfield.sourceFields.clear();
    outfield.sourceFields.add(maxfield);
    outfield.expression = maxfield.name;
    resJoinStep.joinType = T!etl::JoinType#FULL;
// ResultTarget    
    var resultTarget = transformation.createTarget(new T!etl::LocalTarget, "ResultTarget");
    transformation.createInputPort(resultTarget, resJoinStep.outputPort);
    resultTarget.localFileName = deployment.hdfsBaseDirectory + "/" + deployment.hdfsUser + "/datasets/" + dataset.name.toLowerCase() + "_res";
    resultTarget.localFileFormat = T!etl::LocalFileFormat#PARQUET;
    resultTarget.saveMode = T!etl::SaveMode#APPEND;
    resultTarget.deleteBeforeSave = true; 
    resultTarget.partitions = dataset.paramFields().collect(f|f.name.toLowerCase());
    resultTarget.createFieldMappings();
}

@greedy
rule Dataset2TransformationDeployment
    transform dataset: S!Dataset
    to transformationDeployment: T!TransformationDeployment {
    transformationDeployment.name = "autogenerated_tr_" + trName;
    transformationDeployment.project = dataset.project;
    var transformation: T!Transformation ::= dataset;
    transformationDeployment.transformation = transformation;
    transformationDeployment.jobServer = deployment.sparkJobServer;
    transformationDeployment.debug = false; 
    transformationDeployment.slideSize = 400;
    transformationDeployment.rejectSize = 1000;
    transformationDeployment.fetchSize = 1000;
    transformationDeployment.partitionNum = 1;
    transformationDeployment.persistOnDisk = true;
}
