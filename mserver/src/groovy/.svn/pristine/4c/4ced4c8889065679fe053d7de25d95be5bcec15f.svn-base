package MetaServer.etl

import MetaServer.rt.JdbcConnection
import MetaServer.utils.ECoreHelper
import MetaServer.utils.GenerationBase
import org.eclipse.emf.ecore.EObject
import ru.neoflex.meta.utils.Context
/* protected region MetaServer.etlTransformation.inport on begin */
import org.eclipse.epsilon.common.util.StringProperties
import org.eclipse.epsilon.emc.emf.EmfModel
import ru.neoflex.meta.model.Database
import MetaServer.utils.MetaInfo
import MetaServer.utils.SparkSQL
import MetaServer.rt.TransformationDeployment
import org.apache.commons.logging.Log
import org.apache.commons.logging.LogFactory
import ru.neoflex.meta.utils.ECoreUtils
import ru.neoflex.meta.utils.MetaResource

import java.text.DateFormat
import java.text.SimpleDateFormat

import static java.nio.file.Files.readAllBytes
import static java.nio.file.Paths.get

/* protected region MetaServer.etlTransformation.inport end */
class Transformation extends GenerationBase {
    /* protected region MetaServer.etlTransformation.statics on begin */
    private final static Log logger = LogFactory.getLog(Transformation.class)
    private static SimpleDateFormat jsonTimestampFormatter = new SimpleDateFormat("yyyy-MM-dd'T'HH:mm:ss.SSSZ")

    static List collectTrDeployments(Map tr, List collected) {
        (tr.sources.collect {it} + tr.targets.collect {it}).findAll { it.context != null }.collect {
            def ssname = it.context.name
            def ss = Context.current.session.createQuery("from rt.SoftwareSystem where name = :name").setParameter("name", ssname).uniqueResult()
            if (ss == null) {
                throw new RuntimeException("rt.SoftwareSystem ${ssname} not found")
            }
            if (ss.defaultDeployment == null) {
                throw new RuntimeException("default deployment for rt.SoftwareSystem ${ssname} not defined")
            }
            collected.add(ss.defaultDeployment)
        }
        return collected
    }

    static Map findOrCreateAutogeneratedProject() {
        def name = "autogenerated"
        def db = Database.new
        def project = db.session.createQuery("from etl.Project where name = :name").setParameter("name", name).uniqueResult()
        if (project == null) {
            println("create new project ${name}")
            project = db.instantiate("etl.Project", [name: name])
            db.save(project)
        }
        return project
    }

    static Map findOrCreateWF(name, Map entity) {
        def db = Database.new
        def wf = db.session.createQuery("from etl.Workflow where name = :name").setParameter("name", name).uniqueResult()
        if (wf == null) {
            def project = findOrCreateAutogeneratedProject()
            logger.info("create new workflow ${name}")
            wf = db.instantiate("etl.Workflow", [name: name, project: project])
            def end = db.instantiate("etl.WFEnd", [name: "end"])
            def kill = db.instantiate("etl.WFKill", [name: "kill"])
            def transformation = db.instantiate("etl.WFTransformation", [name: entity.name, transformation: entity, ok: end, error: kill])
            def start = db.instantiate("etl.WFManualStart", [name: "start", to: transformation])
            wf.nodes.addAll([start, transformation, end, kill])
            db.save(wf)
        }
        return wf
    }


    static Object generate(Map entity, Map params){
        logger.info("Generate Transformation sources")
        def sourcesDir = getSourcesDirectoryPath("Transformation")
        def transformation = new Database("teneo").get("etl.Transformation", (Long) entity.e_id)
        def auditInfo = transformation.auditInfo
        def changeDateTime = auditInfo.changeDateTime
        def transformationDate = new Date(new File(sourcesDir + "/${transformation.name}", "pom.xml").lastModified())
        if(transformationDate.after(changeDateTime)) {
            return [result: true, fileContent: "Don't need to be generated. All is up to date"]
        }

        def emfModel = new EmfModel()
        def properties = new StringProperties()
        properties.put(EmfModel.PROPERTY_NAME, "src")
        properties.put(EmfModel.PROPERTY_MODEL_URI, "hibernate://?dsname=teneo&query1=from etl.Transformation where e_id=${entity.e_id}")
        properties.put(EmfModel.PROPERTY_METAMODEL_URI, "http://www.neoflex.ru/meta/etl")
        properties.put(EmfModel.PROPERTY_READONLOAD, "true")
        emfModel.load(properties, "")
        Context.current.getContextSvc().epsilonSvc.executeEgx("/psm/etl/spark/Transformation2.egx", [mspaceRoot: "file:///" + sourcesDir,  nodeName: params.nodeName, outputType: params.outputType], [emfModel])
        Transformation.generateTests(entity, params)
        MetaResource.exportDir("psm/etl/spark/src/main/resources", new File("${sourcesDir}/${transformation.name}/src/main/resources"))
        return [result: true, fileContent: "", problems: ""]
    }

    static Object build(Map entity, Map params){
        logger.info("Run Transformation build")
        def database = new Database("teneo")
        def transformation = database.get("etl.Transformation", (Long) entity.e_id)
        def buildFile = getSourcesDirectoryPath("Transformation") + "/" +transformation.get("name") + "/pom.xml"
        def tests = database.session.createQuery("from etl.TransformationTest where transformation.e_id=${entity.e_id}").list()
        def enabledTestsList = new ArrayList()
        def packagePrefix = "ru.neoflex.meta.etl2.spark"
        for(t in tests){
            if(t.enabled){
                enabledTestsList.add(packagePrefix + "." + transformation.name + t.name + "Test")
            }
        }
        def haveEnabledTests = enabledTestsList.size() > 0

        Map map = new HashMap()
        if(haveEnabledTests){
            def testInclString = "" + enabledTestsList.join()
            map.put("suites", testInclString)
        }else{
            map.put("maven.test.skip", "true")
        }
        Context.current.getContextSvc().getMavenSvc().run(new File(buildFile), "clean,install", null, null, null, map)
        return [result: true, fileContent: "", problems: ""]
    }


    static Object generateTests(Map entity, Map params){
        def db = Database.new
        def sourcesDir = getSourcesDirectoryPath("Transformation")
        def tests = db.session.createQuery("from etl.TransformationTest where transformation.e_id=${entity.e_id}").list()
        for(t in tests) {
            if(!t.enabled){
                continue
            }
            EmfModel emfModel = new EmfModel()
            StringProperties properties = new StringProperties()
            properties.put(EmfModel.PROPERTY_NAME, "src")
            properties.put(EmfModel.PROPERTY_MODEL_URI, "hibernate://?dsname=teneo&query1=from etl.TransformationTest where e_id=${t.e_id}")
            properties.put(EmfModel.PROPERTY_METAMODEL_URI, "http://www.neoflex.ru/meta/etl")
            properties.put(EmfModel.PROPERTY_READONLOAD, "true")
            emfModel.load(properties, "")
            Context.current.getContextSvc().epsilonSvc.executeEgx("/psm/etl/spark/TransformationTest.egx", [mspaceRoot: "file:///" + sourcesDir, nodeName: params.nodeName, outputType: params.outputType], [emfModel])
        }
        return [result: true, fileContent: ""]
    }


    static Object checkTRD(Map transformation, Map params = null) {
        def db = Database.new
        def tr = db.get(transformation)
        def trdList = db.session.createQuery("from rt.TransformationDeployment where transformation.e_id=${tr.e_id}").list()
        if (trdList.size() == 0) {
            def trd = createTRD("autogenerated_tr_" + tr.name, tr)
        }
        return [result: true]
    }

    static Map findOrCreateTRD(Map transformation) {
        return findOrCreateTRD("autogenerated_tr_" + transformation.name, transformation)
    }

    static Map findOrCreateTRD(name, Map transformation) {
        def db = Database.new
        def trdList
        trdList = db.session.createQuery("from rt.TransformationDeployment where transformation.e_id=${transformation.e_id}").list()
        if (trdList.size() == 1) {return adjustTRD(trdList.get(0))}
        if (trdList.size() > 1) {
            def defaultTRDs = trdList.findAll {it.isDefault == true}
            if (defaultTRDs.size() > 1) throw new RuntimeException("Multiple default TransformationDeployments for Transformation ${transformation.name} found")
            if (defaultTRDs.size() == 0) throw new RuntimeException("Multiple TransformationDeployments for Transformation ${transformation.name} found. Set default TransformationDeployment")
            return defaultTRDs.get(0)
        }
        if (trdList.size() == 0) {
           return createTRD(name, transformation)
        }
    }

    static adjustTRD(Map trd) {
        def deployments = []
        def count = 0
        collectTrDeployments(trd.transformation, deployments)
        deployments.each { d->
            if (!trd.deployments.any {it.name == d.name}) {
                trd.deployments.add(d)
                count += 1
            }
        }
        if (count > 0) {
            Database.new.save(trd)
        }
        return trd
    }

    static Map createTRD(name, Map transformation) {
        logger.info("create new TransformationDeployment ${name}")
        def project = transformation.project
        def livyServer = null
        if (project != null) {
            livyServer = Context.current.session.createQuery("from rt.LivyServer where project.e_id=${project.e_id}").uniqueResult()
        }
        if (livyServer == null) {
            livyServer = Context.current.session.createQuery("from rt.LivyServer where isDefault=true").uniqueResult()
        }
        if (livyServer == null)  {
            throw new RuntimeException("Default LivyServer and LivyServer for project ${project?.name} not found")
        }
        if (project == null) {
            project = Project.findOrCreateProject()
        }
        def deployments = []
        collectTrDeployments(transformation, deployments)
        def db = Database.new
        def props = [name: name, project: project, livyServer: livyServer, transformation: transformation,
                     deployments: deployments.unique {"${it._type_}|${it.e_id}"}, debug: true, slideSize: 400,
                     rejectSize: 1000, fetchSize: 1000, partitionNum: 1, persistOnDisk: true,
                     master: "yarn", mode: "cluster", isDefault: true,
                     driverMemory: livyServer.driverMemory, executorMemory: livyServer.executorMemory,
                     executorCores: livyServer.executorCores, numExecutors: livyServer.numExecutors]
        def trd = db.instantiate("rt.TransformationDeployment", props)
        db.save(trd)
        return trd
    }

    static Object validateModel(Map entity) {
        def fileName = "/pim/etl/etl.evl"
        def emfModel = new EmfModel()
        def properties = new StringProperties()
        properties.put(EmfModel.PROPERTY_NAME, "src")
        properties.put(EmfModel.PROPERTY_MODEL_URI, "hibernate://?dsname=teneo&query1=from etl.Transformation where e_id=${entity.e_id}")
        properties.put(EmfModel.PROPERTY_METAMODEL_URI, "http://www.neoflex.ru/meta/etl")
        properties.put(EmfModel.PROPERTY_READONLOAD, "true")
        emfModel.load(properties, "" )
        def problems = []
        Context.current.getContextSvc().epsilonSvc.executeEvl(fileName, [:], [emfModel], problems)
        return [result: !problems.any{it.isCritique == false}, problems: problems]
    }

    static Object validateScripts(Map entity) {
        def loaded = new Database("teneo").get("etl.Transformation", (Long)entity.e_id)
        Database.new.refresh(loaded)
        loaded.values()
        def problems = []
        validateProjections(loaded, problems)
        validateSelections(loaded, problems)
        validateJoins(loaded, problems)
        validateExpressionSources(loaded, problems)
        validateAggregation(loaded, problems)
        validateOutputPorts(loaded, problems)
        return [result: (problems.find {it.isCritique == false} == null), problems: problems]
    }

    private static boolean validateProjections(Map entity, List problems) {
        int errors = 0
        for (step in entity.transformationSteps) {
            if (step._type_ == "etl.Projection") {
                for (field in step.outputPort.fields) {
                    if ("TRANSFORM" == field.fieldOperationType.toString()) {
                        //def testResult = [result: true]
                        def testResult = ProjectionField.test(field)
                        if (testResult.result != true) {
                            problems.add( [
                                    message: testResult.message,
                                    isCritique: false,
                                    context: "ProjectionField.${step.name}.${field.name}".toString(),
                                    constraint: "CheckScript",
                                    _type_: step._type_,
                                    e_id: step.e_id
                            ])
                            errors += 1
                        }
                    }
                }
            }
        }
        return errors > 0
    }

    private static boolean validateOutputPorts(Map entity, List problems) {
        int errors = 0
        def steps = entity.transformationSteps.collect {it} + entity.sources.collect {it}
        for (step in steps) {
            for (debug in step.outputPort.debugList) {
                def testResult = DebugOutput.test(debug)
                if (testResult.result != true) {
                    problems.add( [
                            message: testResult.message,
                            isCritique: false,
                            context: "DebugOutput(${entity.name}.${step.name}.${debug.name})".toString(),
                            constraint: "CheckScript",
                            _type_: step._type_,
                            e_id: step.e_id
                    ])
                    errors += 1
                }
            }
        }
        return errors > 0
    }

    private static boolean validateJoins(Map entity, List problems) {
        int errors = 0
        for (step in entity.transformationSteps) {
            if (step._type_ == "etl.Join") {
                for (field in step.outputPort.fields) {
                    if ("TRANSFORM" == field.fieldOperationType.toString()) {
                        def testResult = ProjectionField.test(field)
                        if (testResult.result != true) {
                            problems.add( [
                                    message: testResult.message,
                                    isCritique: false,
                                    context: "ProjectionField.${entity.name}.${field.name}".toString(),
                                    constraint: "CheckScript",
                                    _type_: step._type_,
                                    e_id: step.e_id
                            ])
                            errors += 1
                        }
                    }
                }
            }
        }
        return errors > 0
    }

    private static boolean validateSelections(Map entity, List problems) {
        int errors = 0
        for (step in entity.transformationSteps) {
            if (step._type_ == "etl.Selection") {
                def testResult = Selection.test(step)
                if (testResult.result != true) {
                    problems.add( [
                            message: testResult.message,
                            isCritique: false,
                            context: "Selection.${entity.name}".toString(),
                            constraint: "CheckScript",
                            _type_: step._type_,
                            e_id: step.e_id
                    ])
                    errors += 1
                }
            }
        }
        return errors > 0
    }

    private static boolean validateExpressionSources(Map entity, List problems) {
        int errors = 0
        for (step in entity.transformationSteps) {
            if (step._type_ == "etl.ExpressionSource") {
                def testResult = ExpressionSource.test(step)
                if (testResult.result != true) {
                    problems.add( [
                            message: testResult.message,
                            isCritique: false,
                            context: "ExpressionSource.${entity.name}".toString(),
                            constraint: "CheckScript",
                            _type_: step._type_,
                            e_id: step.e_id
                    ])
                    errors += 1
                }
            }
        }
        return errors > 0
    }

    private static boolean validateAggregation(Map entity, List problems) {
        int errors = 0
        for (step in entity.transformationSteps) {
            if (step._type_ == "etl.Aggregation" && step.aggregationFunction.toString() == "USER_DEF") {
                def testResult = Aggregation.testExpression(step)
                if (testResult.result != true) {
                    problems.add( [
                            message: testResult.message,
                            isCritique: false,
                            context: "ExpressionSource.${entity.name}".toString(),
                            constraint: "CheckExpression",
                            _type_: step._type_,
                            e_id: step.e_id
                    ])
                    errors += 1
                }
                testResult = Aggregation.testInitExpression(step)
                if (testResult.result != true) {
                    problems.add( [
                            message: testResult.message,
                            isCritique: false,
                            context: "ExpressionSource.${entity.name}".toString(),
                            constraint: "CheckInitExpression",
                            _type_: step._type_,
                            e_id: step.e_id
                    ])
                    errors += 1
                }
                testResult = Aggregation.testFinalExpression(step)
                if (testResult.result != true) {
                    problems.add( [
                            message: testResult.message,
                            isCritique: false,
                            context: "ExpressionSource.${entity.name}".toString(),
                            constraint: "CheckFinalExpression",
                            _type_: step._type_,
                            e_id: step.e_id
                    ])
                    errors += 1
                }
            }
        }
        return errors > 0
    }


    static Object validateTests(Map transformation, Map params =null) {
        def fileName = "/pim/etl/etl.evl"

        def db = Database.new
        def testList
        testList = db.session.createQuery("from etl.TransformationTest where transformation.e_id=${transformation.e_id}").list()
        def problems = []
        for(t in testList){
            def emfModel = new EmfModel()
            def properties = new StringProperties()
            properties.put(EmfModel.PROPERTY_NAME, "src")
            properties.put(EmfModel.PROPERTY_MODEL_URI, "hibernate://?dsname=teneo&query1=from etl.TransformationTest where e_id=${t.e_id}")
            properties.put(EmfModel.PROPERTY_METAMODEL_URI, "http://www.neoflex.ru/meta/etl")
            properties.put(EmfModel.PROPERTY_READONLOAD, "true")
            emfModel.load(properties, "" )
            Context.current.getContextSvc().epsilonSvc.executeEvl(fileName, [:], [emfModel], problems)
        }
        return [result: !problems.any{it.isCritique == false}, problems: problems]
    }

    private static Map inputFieldDependencies(Map transformation, Map field) {
        def outputPort = findOutputPortByInputPort(transformation, field.dataSet)
        def innode = findNodeByOutputPort(transformation, outputPort)
        fieldDependencies(transformation, innode, innode.outputPort.fields.find {it.name == field.name})
    }

    private static List getInputPorts(Map node) {
        def result = []
        if (node.inputPort != null) result.add(node.inputPort)
        if (node.sqlPorts != null) result.addAll(node.sqlPorts)
        if (node.joineePort != null) result.add(node.joineePort)
        if (node.unionPort != null) result.add(node.unionPort)
        return result
    }

    private static Map outputFieldFollowers(Map transformation, Map node, Map field) {
        def result = [fieldName: field.name, fieldType: field.dataTypeDomain.toString() , nodeName: node.name, followers: []]
        findInputPortsByOutputPort(transformation, field.dataSet).each {
            def outnode = findNodeByInputPort(transformation, it)
            def planDep = null
            if (outnode._type_ == "etl.SparkSQL") {
                try {
                    planDep = SparkSQL.extractDepsFromSQL(outnode.statement)
                }
                catch (th) {
                    println("error parsing SparkSQL: " + th.toString())
                }
            }
            def infield = it.fields.find {it.name == field.name}
            if (outnode.outputPort != null) {
                if (planDep != null) {
                    result.followers.addAll(outnode.outputPort.fields.findAll {outField->
                        (planDep[outField.name] ?: []).any {
                            it.name.equalsIgnoreCase(infield.name) && it.alias.equalsIgnoreCase(infield.dataSet.alias)
                        } || (planDep["*"] ?: []).any {
                            outField.name.equalsIgnoreCase(infield.name) && it.alias.equalsIgnoreCase(infield.dataSet.alias)
                        }
                    }.collect {
                        outputFieldFollowers(transformation, outnode, it)
                    })
                }
                else {
                    result.followers.addAll(outnode.outputPort.fields.findAll {it._type_ == 'dataset.Field' && it.name == infield.name}.collect {
                        outputFieldFollowers(transformation, outnode, it)
                    })
                    result.followers.addAll(outnode.outputPort.fields.findAll {it._type_ == 'etl.ProjectionField' && it.sourceFields.any {it.name == infield.name}}.collect {
                        outputFieldFollowers(transformation, outnode, it)
                    })
                    result.followers.addAll(outnode.outputPort.fields.findAll {it._type_ == 'etl.UnionField' && it.inputPortField?.e_id == infield.e_id}.collect {
                        outputFieldFollowers(transformation, outnode, it)
                    })
                    result.followers.addAll(outnode.outputPort.fields.findAll {it._type_ == 'etl.UnionField' && it.unionPortField?.e_id == infield.e_id}.collect {
                        outputFieldFollowers(transformation, outnode, it)
                    })
                }
            }
            else {
                result.followers.add([fieldName: infield.name, fieldType: infield.dataTypeDomain.toString() , nodeName: outnode.name, followers: []])
            }
        }
        return result
    }

    private static Map fieldDependencies(Map transformation, Map node, Map field) {
        def result = [fieldName: field.name, fieldType:field.dataTypeDomain.toString() , nodeName: node.name, dependencies: []]
        if (field._type_ == "dataset.Field") {
            if (node.inputPort != null) {
                def inputFields = node.inputPort.fields.findAll {it.name == field.name}
                result.dependencies.addAll(inputFields.collect {
                    inputFieldDependencies(transformation, it)
                })
            }
            if (node.sqlPorts != null) {
                try {
                    def planDeps = SparkSQL.extractDepsFromSQL(node.statement)
                    def deps = planDeps[field.name] ?: []
                    def starDeps = planDeps["*"]
                    if (starDeps != null) {
                        deps.addAll(starDeps.collect {[alias: it.alias, name: field.name]})
                    }
                    deps.each {dep->
                        node.sqlPorts.findAll {sqlPort->
                            sqlPort.alias.equalsIgnoreCase(dep.alias)
                        }.each {sqlPort->
                            result.dependencies.addAll(sqlPort.fields.findAll {it.name.equalsIgnoreCase(dep.name)}.collect {
                                inputFieldDependencies(transformation, it)
                            })
                        }

                    }
                }
                catch (th) {
                    println("error parsing sql - compare field names")
                    for (sqlPort in node.sqlPorts) {
                        def inputFields = sqlPort.fields.findAll {it.name == field.name}
                        result.dependencies.addAll(inputFields.collect {
                            inputFieldDependencies(transformation, it)
                        })
                    }
                }
            }
        }
        else if (field._type_ == "etl.ProjectionField") {
            result.dependencies.addAll(field.sourceFields.collect {
                inputFieldDependencies(transformation, it)
            })
        }
        else if (field._type_ == "etl.UnionField") {
            if (field.inputPortField != null) {
                result.dependencies.add(inputFieldDependencies(transformation, field.inputPortField))
            }
            if (field.unionPortField != null) {
                result.dependencies.add(inputFieldDependencies(transformation, field.unionPortField))
            }
        }
        return result
    }

    private static Map findNodeByOutputPort(Map transformation, Map outputPort) {
        return ([] + transformation.sources + transformation.transformationSteps).find {it.outputPort.e_id == outputPort.e_id}
    }

    private static Map findNodeByInputPort(Map transformation, Map inputPort) {
        return ([] + transformation.transformationSteps + transformation.targets).find {getInputPorts(it).contains(inputPort)}
    }

    private static Map findOutputPortByInputPort(Map transformation, Map inputPort) {
        return transformation.transitions.findAll {it.finish.e_id == inputPort.e_id}.collect{it.start}.first()
    }

    private static List findInputPortsByOutputPort(Map transformation, Map outputPort) {
        return transformation.transitions.findAll {it.start.e_id == outputPort.e_id}.collect{it.finish}
    }

    static Object expandXMLFile(Map entity, Map params = null) {
      def result = []
      def tr = Database.new.get(entity)
      def trd = findOrCreateTRD(tr)
      Context.current.commit()

      def code = Context.current.getContextSvc().epsilonSvc.executeEgl("/psm/etl/spark/SourceStructureExpand.egl", [format: "xml", fileName: entity.fileName, rowTag: entity.rowTag, sampleSize: entity.sampleSize, dontExplode: entity.dontExplode], [])
      return TransformationDeployment.runPart(trd, [code: code, outputType: 'json', sessionId: entity.sessionId])
    }

    static Object expandAvroFile(Map entity, Map params = null) {
      def result = []
      def tr = Database.new.get(entity)
      def trd = findOrCreateTRD(tr)
      Context.current.commit()

      def code = Context.current.getContextSvc().epsilonSvc.executeEgl("/psm/etl/spark/SourceStructureExpand.egl", [format: "avro", fileName: entity.fileName, rowTag: entity.rowTag, sampleSize: entity.sampleSize, dontExplode: entity.dontExplode], [])
      return TransformationDeployment.runPart(trd, [code: code, outputType: 'json', sessionId: entity.sessionId])
    }

    static Object importTransformation(Map entity, Map params = null) {
        def transformation = Database.new.get(entity)
        /*def project = transformation.project
        if (project == null) {
            throw new RuntimeException("Project not found")
        }*/
        return importTransformationInternal(entity, params)
    }


    static Object importTransformationInternal(Map entity, Map params = null){

        def gitFlow = Context.current.getContextSvc().getGitflowSvc()
        def projectDir = gitFlow.getBranchDir(gitFlow.getCurrentBranch())
        def xmiFiles = new ArrayList<File>()
        def refFiles = new ArrayList<File>()
        projectDir.traverse(type: groovy.io.FileType.FILES) { it ->
            if(it.name.toLowerCase().endsWith(entity.get("name") + ".xmi")){
                xmiFiles.add(it)
            }
        }
        projectDir.traverse(type: groovy.io.FileType.FILES) { it ->
            if (it.name.toLowerCase().endsWith(entity.get("name") + ".ref")) {
                refFiles.add(it)
            }
        }
        for(File f : xmiFiles){
            def imported = gitFlow.importXmiFile(f)
        }

        for(File f : refFiles){
            def imported = gitFlow.importRefFile(f)
        }

        def resList = new ArrayList<File>()
        resList.addAll(xmiFiles)
        resList.addAll(refFiles)
        def join = resList.join(",\r\n")
        return [result: true, fileContent: join]
    }

    static Object exportTransformation(Map entity, Map params = null) {
        def transformation = Database.new.get(entity)
        def project = transformation.project
        /*if (project == null) {
            throw new RuntimeException("Project not found")
        }*/
        def svnCommitMessage = params.svnCommitMessage
        return exportProjectEntityWithDependentObjects(transformation, svnCommitMessage)
    }


    static List exportProjectEntityWithDependentObjects(Map entity, svnCommitMessage) {
        def eCoreHelper = new ECoreHelper()
        def files = new ArrayList<File>()
        def filesDeps = eCoreHelper.getAllDependentObjectsOfEntity(entity)
        def gitFlow = Context.current.getContextSvc().getGitflowSvc()
        //def branchDir = gitFlow.getBranchDir(gitFlow.getCurrentBranch())
        for(Object e : filesDeps){
            //Map merged = ECoreUtils.merge(null, e);
            def ent= Database.new.get(e)
            List<File> res  = gitFlow.exportEObject((EObject)ent)
            files.addAll(res)
        }
        generate(entity, null)
        //gitFlow.commit(gitFlow.getCurrentBranch(), files, svnCommitMessage);
        gitFlow.commit(gitFlow.getCurrentBranch(), svnCommitMessage)
        return files
    }
    /* protected region MetaServer.etlTransformation.statics end */

    static Object validate(Map entity, Map params = null) {
    /* protected region MetaServer.etlTransformation.validate on begin */
        def retModel = validateModel(entity)
        def retScripts = validateScripts(entity)
        def ret = [result: retModel.result && retScripts.result, problems: retModel.problems + retScripts.problems]
        for (problem in ret.problems) {
            logger.warn(problem)
        }
        return ret
    /* protected region MetaServer.etlTransformation.validate end */
    }

    static Object dependencies(Map entity, Map params = null) {
    /* protected region MetaServer.etlTransformation.dependencies on begin */
        def deps = MetaInfo.getAllDeps(entity).collect {[_type_: it._type_, name: it.name, e_id: it.e_id]}
        println(deps)
        return deps
    /* protected region MetaServer.etlTransformation.dependencies end */
    }

    static Object setJsonView(Map entity, Map params = null) {
    /* protected region MetaServer.etlTransformation.setJsonView on begin */
        Project.setEntityJsonView(entity._type_, entity.e_id, true)
        return ["Hello from MetaServer.etl.Transformation.setJsonView"]
    /* protected region MetaServer.etlTransformation.setJsonView end */
    }

    static Object install(Map entity, Map params = null) {
    /* protected region MetaServer.etlTransformation.install on begin */
        def tr = Database.new.get(entity)
        def trd = findOrCreateTRD(tr)
        Context.current.commit()
        return TransformationDeployment.install(trd, params)
    /* protected region MetaServer.etlTransformation.install end */
    }

    static Object runit(Map entity, Map params = null) {
    /* protected region MetaServer.etlTransformation.runit on begin */
        def trd = entity.trd ? Database.new.get(entity.trd) : findOrCreateTRD(Database.new.get(entity))
        Context.current.commit()
        def allParams = entity.params ?: [:] as Map
        allParams.putAll(params ?: [:])
        return TransformationDeployment.generateAndRun(trd, allParams)
    /* protected region MetaServer.etlTransformation.runit end */
    }

    static Object runPart(Map entity, Map params = null) {
    /* protected region MetaServer.etlTransformation.runit on begin */
        def tr = Database.new.get(entity)
        def trd = findOrCreateTRD(tr)
        Context.current.commit()
        def outputType
        if (entity.outputType == null) outputType = 'text' else outputType = entity.outputType
        return TransformationDeployment.runPart(trd, [livyServer: entity.server, code: entity.fileContent, sessionId: entity.sessionId, outputType: outputType])
    /* protected region MetaServer.etlTransformation.runit end */
    }

    static Object partJobFile(Map entity, Map params = null) {
    /* protected region MetaServer.etlTransformation.runit on begin */
      def result = []
      def tr = Database.new.get(entity)
      def trd = findOrCreateTRD(tr)
      Context.current.commit()
      def outputType
      if (entity.outputType == null) outputType = 'text' else outputType = entity.outputType
      return TransformationDeployment.generatePart(trd, [nodeName: entity.nodeName, outputType: outputType, sampleSize: entity.sampleSize])
    /* protected region MetaServer.etlTransformation.runit end */
    }

    static Object generateAndRunPart(Map entity, Map params = null) {
    /* protected region MetaServer.etlTransformation.runit on begin */
      def result = []
      def tr = Database.new.get(entity)
      def trd = findOrCreateTRD(tr)
      Context.current.commit()
      def outputType
      if (entity.outputType == null) outputType = 'text' else outputType = entity.outputType
      def fileContent = TransformationDeployment.generatePart(trd, [nodeName: entity.nodeName, outputType: outputType, sampleSize: entity.sampleSize, statement: entity.statement]).fileContent
      return TransformationDeployment.runPart(trd, [code: fileContent, outputType: outputType, sessionId: entity.sessionId])
    /* protected region MetaServer.etlTransformation.runit end */
    }

    static Object mainJobFile(Map entity, Map params = null) {
    /* protected region MetaServer.etlTransformation.runit on begin */
    	def result = []
      def tr = Database.new.get(entity)
      def trd = findOrCreateTRD(tr)
      Context.current.commit()
      return TransformationDeployment.readFile(trd, params)
    /* protected region MetaServer.etlTransformation.runit end */
    }

    static Object writeMainJobFile(Map entity, Map params = null) {
    /* protected region MetaServer.etlTransformation.runit on begin */
    	def result = []

      def tr = Database.new.get(entity)
      def trd = findOrCreateTRD(tr)
      Context.current.commit()
      return TransformationDeployment.writeFile(trd, params, entity.fileContent)
    /* protected region MetaServer.etlTransformation.runit end */
    }

    static Object nodeDependencies(Map entity, Map params = null) {
    /* protected region MetaServer.etlTransformation.nodeDependencies on begin */
        def transformation_Id = entity?.transformation_Id ?: entity?.e_id ?: params?.transformation_Id
        def node_name = entity?.node_name ?: params?.node_name
        def transformation = Database.new.get([_type_:"etl.Transformation", e_id: transformation_Id])
        def dependencies = []
        def followers = []
        def node = ([] + transformation.sources + transformation.transformationSteps).find {it.name == node_name}
        if (node == null) {
            node = transformation.targets.find {it.name == node_name}
            if (node == null) {
                throw new RuntimeException("${node_name} not found")
            }
            dependencies = node.inputPort.fields.collect{fieldDependencies(transformation, node, it)}
            followers = node.inputPort.fields.collect{outputFieldFollowers(transformation, node, it)}
        }
        else {
            dependencies = node.outputPort.fields.collect{fieldDependencies(transformation, node, it)}
            followers = node.outputPort.fields.collect{outputFieldFollowers(transformation, node, it)}
        }
        SparkSQL.clearPlanCache()
        return [dependencies: dependencies, followers: followers]
    /* protected region MetaServer.etlTransformation.nodeDependencies end */
    }

    static Object svnProps(Map entity, Map params = null) {
        def transformation = Database.new.get(entity)
        def project = transformation.project
        return Project.svnProps(project, transformation)
    }
}
