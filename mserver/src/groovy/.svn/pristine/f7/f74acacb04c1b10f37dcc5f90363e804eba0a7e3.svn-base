package MetaServer.rt

import MetaServer.etl.Transformation
import MetaServer.utils.EMF
import MetaServer.utils.FileSystem
import MetaServer.utils.GenerationBase
import MetaServer.utils.HDFSClient
import org.apache.commons.logging.Log
import org.apache.commons.logging.LogFactory
import org.eclipse.epsilon.common.util.StringProperties
import org.eclipse.epsilon.emc.emf.EmfModel
import ru.neoflex.meta.model.Database
import ru.neoflex.meta.utils.Context

/* protected region MetaServer.rtTransformationDeployment.inport on begin */
import ru.neoflex.meta.utils.JSONHelper
import ru.neoflex.meta.utils.SymmetricCipher

import java.nio.file.Paths

import static java.nio.file.Files.readAllBytes
import static java.nio.file.Files.write
import static java.nio.file.Paths.get

/* protected region MetaServer.rtTransformationDeployment.inport end */

class TransformationDeployment extends GenerationBase {
    /* protected region MetaServer.rtTransformationDeployment.statics on begin */
    private final static Log logger = LogFactory.getLog(TransformationDeployment.class)

    static Object chain(fs, Map entity, Map params = null) {
        def problems = []
        def data = params ?: [:]
        for (f in fs) {
            def ret = f(entity, data)
            problems += ret.problems
            data.putAll(ret.data ?: [:])
            if (!ret.result) return [result: false, problems: problems, data: data]
        }
        return [result: true, problems: problems, data: data]
    }

    static Object validateModel(Map entity, Map params = null) {
        def trDeployment = Database.new.get(entity)
        def problems = []
        Transformation.validateModel(trDeployment.transformation)
        return [result: problems.find { it.isCritique == false } == null, problems: problems]
    }

    static Object getLastUpdated(Map e) {
        if (e.auditInfo == null) {
            return null
        }
        return e.auditInfo.changeDateTime == null ? e.auditInfo.createDateTime : e.auditInfo.changeDateTime
    }

    static Object getMaxUpdated(List ents) {
        def result = null
        ents.each {
            def lastUpdated = getLastUpdated(it)
            if (result == null || lastUpdated != null && lastUpdated.after(result)) {
                result = lastUpdated
            }
        }
        return result
    }

    static Object generateAndRunNoWait(Map entity, Map params = null) {
        return chain([
                TransformationDeployment.&install,
                TransformationDeployment.&run
        ], entity, params)
    }

    static Object waitJob(Map entity, Map params = null) {
        def sessionState = 'running'
        def problems = []
        while (sessionState != 'error' && sessionState != 'dead' && sessionState != 'success') {
            sessionState = LivyServer.checkBatchSession(Database.new.get(entity).livyServer, [sessionId: params.sessionId]).result
            sleep(params.timeout ?: 500)
        }
        def message = "Batch ${params.sessionId} finished with status ${sessionState}\nSee Batches in Livy Console for details".toString()
        if (sessionState != 'success') {
            problems += [isCritique: true, constraint: "BatchSuccess", context: "${entity._type_}[${entity.e_id}]".toString(), message: message]
        }
        logger.info(message)
        return [result: sessionState == 'success', problems: problems]
    }

    static Object readFile(Map entity, Map params = null) {
        def generateResult = chain([
                TransformationDeployment.&generate
        ], entity, params)
        if (generateResult.result) {
            def deployDir = Context.current.getContextSvc().getDeployDir().getAbsolutePath()
            def sourcesDir = Context.current.getContextSvc().getGitflowSvc().getCurrentSourcesDir().getAbsolutePath() + "/TransformationDeployment"
            def trDeployment = new Database("teneo").get("rt.TransformationDeployment", (Long) entity.e_id)
            def transformation = trDeployment.transformation
            File file = new File(sourcesDir, "${trDeployment.name}/${transformation.name}/src/main/scala/${transformation.name}Job.scala")
            return [result: true, fileContent: new String(readAllBytes(get(file.getPath())))]
        } else {
            return generateResult
        }
    }

    static boolean writeFile(Map entity, Map params = null, String fileContent) {
        Database db = Database.new
        Map td = db.get(entity)
        def deployDir = Context.current.getContextSvc().getmSpaceSvc().getEpsilonSvc().getDeployDir().getAbsolutePath()
        def sourcesDir = Context.current.getContextSvc().getGitflowSvc().getCurrentSourcesDir().getAbsolutePath() + "/TransformationDeployment"

        File statusFile = new File(deployDir, "deployments/${td.name}/status.txt")
        def generateResult = [result: true]
        if (!statusFile.isFile()) {
            generateResult = chain([
                    TransformationDeployment.&generate
            ], entity, params)
        }
        if (generateResult.result) {
            deployDir = Context.current.getContextSvc().getDeployDir().getAbsolutePath()
            def trDeployment = new Database("teneo").get("rt.TransformationDeployment", (Long) entity.e_id)
            def transformation = trDeployment.transformation
            File file = new File(sourcesDir, "${trDeployment.name}/${transformation.name}/src/main/scala/${transformation.name}Job.scala")
            write(Paths.get(file.getPath()), fileContent.getBytes())
            return [result: true, fileContent: new String(readAllBytes(get(file.getPath())))]
        } else {
            return generateResult
        }
    }

    /* protected region MetaServer.rtTransformationDeployment.statics end */

    static Object validate(Map entity, Map params = null) {
        /* protected region MetaServer.rtTransformationDeployment.validate on begin */
        def trDeployment = Database.new.get(entity)
        def retTD = EMF.validate(trDeployment, "/pim/rt/rt.evl")
        def retT = trDeployment.transformation != null ? Transformation.validate(trDeployment.transformation) : [result: true, problems: []]
        def result = [result: retTD.result && retT.result, problems: retTD.problems + retT.problems]
        if (!result.result) {
            logger.warn(result.problems.collect { it.message }.join("\n"))
        }
        return result
        /* protected region MetaServer.rtTransformationDeployment.validate end */
    }

    static String getDeploymentDir(Map trd) {
        //def deployDir = Context.current.getContextSvc().getmSpaceSvc().getEpsilonSvc().getDeployDir().getAbsolutePath()
        def sourcesDir = getSourcesDirectoryPath("TransformationDeployment")
        return sourcesDir + "/${trd.name}".toString()
    }

    static String getSrcDir(Map trd) {
        return "${getDeploymentDir(trd)}/${trd.transformation.name}".toString()
    }

    static Object generate(Map entity, Map params = null) {
        /* protected region MetaServer.rtTransformationDeployment.generate on begin */
        def retModel = validateModel(entity)
        if (!retModel.result) {
            for (problem in retModel.problems) {
                logger.error(problem)
            }
            return retModel
        }
        def sourcesDir = getSourcesDirectoryPath("TransformationDeployment")
        def trDeployment = new Database("teneo").get("rt.TransformationDeployment", (Long) entity.e_id)
        def dd = getDeploymentDir(trDeployment)
        FileSystem.forceDeleteFolder(Paths.get(dd))
        def transformation = trDeployment.transformation
        println(transformation.name)
        //MetaResource.exportDir("psm/etl/spark/src", new File("${deployDir}/deployments/${trDeployment.name}/${transformation.name}/src"))
        //def sd = getSrcDir(trDeployment)
        //MetaResource.exportDir("psm/etl/spark/src/main/resources", new File("${sd}/src/main/resources"))
        def emfModel = new EmfModel()
        def properties = new StringProperties()
        properties.put(EmfModel.PROPERTY_NAME, "src")
        properties.put(EmfModel.PROPERTY_MODEL_URI, "hibernate://?dsname=teneo&query1=from etl.Transformation where e_id=${transformation.e_id}")
        properties.put(EmfModel.PROPERTY_METAMODEL_URI, "http://www.neoflex.ru/meta/etl")
        properties.put(EmfModel.PROPERTY_READONLOAD, "true")
        emfModel.load(properties, "")
        def sparkVer = ""
        if (trDeployment.livyServer) sparkVer = "2"
        //Context.current.getContextSvc().epsilonSvc.executeEgx("/psm/etl/spark/Transformation${sparkVer}.egx", [mspaceRoot: "file:///" + sourcesDir, jobDeployment: trDeployment, packagePrefix: ""], [emfModel])
        //regenerate transformation
        Transformation.generate(transformation, params)
        //config file
        LinkedHashMap<String, Object> batchParams = getBatchParams(trDeployment, trDeployment.transformation, params)
        def config = EMF.generate([trDeployment], "/psm/etl/airflow/config.egl", [batchParams: batchParams])
        def generationFolder = new File(sourcesDir + "/${trDeployment.name}/${trDeployment.transformation.name}")
        if(!generationFolder.exists()){
            generationFolder.mkdirs();
        }
        def configFile = new File(generationFolder, "${trDeployment.transformation.name}.json")
        if(!configFile.exists()){
            configFile.createNewFile()
        }
        configFile << config
        return [result: true, problems: []]
        /* protected region MetaServer.rtTransformationDeployment.generate end */
    }

    static Object generatePart(Map entity, Map params = null) {
        /* protected region MetaServer.rtTransformationDeployment.generatePart on begin */
        def retModel = validateModel(entity)
        if (!retModel.result) {
            for (problem in retModel.problems) {
                logger.error(problem)
            }
            return retModel
        }
        def deployDir = Context.current.getContextSvc().getmSpaceSvc().getEpsilonSvc().getDeployDir().getAbsolutePath()
        def trDeployment = new Database("teneo").get("rt.TransformationDeployment", (Long) entity.e_id)
        def transformation = trDeployment.transformation
        File file = new File(deployDir, "deployments/${trDeployment.name}/${transformation.name}Part.scala")
        file.delete()
        def emfModel = new EmfModel()
        def properties = new StringProperties()
        properties.put(EmfModel.PROPERTY_NAME, "src")
        properties.put(EmfModel.PROPERTY_MODEL_URI, "hibernate://?dsname=teneo&query1=from etl.Transformation where e_id=${transformation.e_id}")
        properties.put(EmfModel.PROPERTY_METAMODEL_URI, "http://www.neoflex.ru/meta/etl")
        properties.put(EmfModel.PROPERTY_READONLOAD, "true")
        emfModel.load(properties, "")
        def workflowId = "${transformation.name}_${System.identityHashCode([])}"
        def jobParams = [
                "HOME=${trDeployment.livyServer.home}".toString(),
                "USER=${trDeployment.livyServer.user}".toString(),
                "WF_HOME=${trDeployment.livyServer.home}/${trDeployment.livyServer.user}".toString(),
                "ROOT_WORKFLOW_ID=${workflowId}".toString(),
                "CURRENT_WORKFLOW_ID=${workflowId}".toString(),
                "SLIDE_SIZE=${trDeployment.slideSize}".toString(),
                "FETCH_SIZE=${trDeployment.fetchSize}".toString(),
                "PARTITION_NUM=${trDeployment.partitionNum}".toString(),
                "FAIL_THRESHOLD=${trDeployment.rejectSize}".toString(),
                "DEBUG=${trDeployment.debug}".toString()
        ]

        def deployments = []
        trDeployment.deployments.each {
            def deployment = [
                    NAME    : "${it.softwareSystem.name}".toString(),
                    URL     : "${it.connection.url}".toString(),
                    USER    : "${it.connection.user}".toString(),
                    DRIVER  : "${it.connection.driver}".toString(),
                    PASSWORD: "${JdbcConnection.getPassword(it.connection)}".toString(),
                    SCHEMA  : "${it.connection.schema}".toString()
            ]
            deployments.add(deployment)
        }
        trDeployment.parameters.each {
            jobParams.add(JSONHelper.escape("${it.name}=${it.value}").toString())
        }

        Context.current.getContextSvc().epsilonSvc.executeEgx("/psm/etl/spark/TransformationPart.egx", [mspaceRoot: "file:///" + deployDir, jobDeployment: trDeployment, nodeName: params.nodeName, outputType: params.outputType, jobParams: jobParams, deployments: deployments, sampleSize: params.sampleSize, statement: params.statement], [emfModel])
        return [result: true, fileContent: new String(readAllBytes(get(file.getPath())))]
        /* protected region MetaServer.rtTransformationDeployment.generatePart end */
    }

    static Object build(Map entity, Map params = null) {
        /* protected region MetaServer.rtTransformationDeployment.build on begin */
        if (params.noBuild != true) {
            def trDeployment = new Database("teneo").get("rt.TransformationDeployment", (Long) entity.e_id)
            def transformation = trDeployment.transformation
            if(isNeedRebuild(trDeployment)){
                def result = chain([
                        Transformation.&generate,
                        Transformation.&build
                ], transformation, params)
            }

            def sourcesDir = Context.current.getContextSvc().getGitflowSvc().getCurrentSourcesDir();
            def transformationDeploymentPath = Context.current.getContextSvc().getGitflowSvc().getCurrentBuildDir().getAbsolutePath() + "/TransformationDeployment"
            /*def buildDir = new File(transformationDeploymentPath, trDeployment.name);
            def currentTrdDir = new File(sourcesDir.getAbsolutePath() + "/TransformationDeployment" + "/" + trDeployment.name);
            def currentTrDir =  new File(sourcesDir.getAbsolutePath() + "/Transformation" + "/" + transformation.name);
            FileSystem.copyFolder(currentTrdDir.toPath(), buildDir.toPath())
            buildDir = new File(buildDir, transformation.name)
            FileSystem.copyFolder(currentTrDir.toPath(), buildDir.toPath())*/
            //Context.current.getContextSvc().getMavenSvc().run(new File(transformationDeploymentPath, "${trDeployment.name}/${transformation.name}/pom.xml"), "clean,install", null, null, null, [:])
        }
        return [result: true, problems: []]
        /* protected region MetaServer.rtTransformationDeployment.build end */
    }

    static Object deploy(Map entity, Map params = null) {
        /* protected region MetaServer.rtTransformationDeployment.deploy on begin */
        def deployDir = Context.current.getContextSvc().getmSpaceSvc().getDeployDir().getAbsolutePath()
        def buildDir = Context.current.getContextSvc().getGitflowSvc().getCurrentBuildDir().getAbsolutePath()
        def trDeployment = new Database("teneo").get("rt.TransformationDeployment", (Long) entity.e_id)
        String deploymentDir = "${buildDir}/TransformationDeployment/${entity.get("name")}"
        def livyServer = trDeployment.livyServer
        def hdfs = new HDFSClient(livyServer.webhdfs, livyServer.user, livyServer)

        def path = "${livyServer.home}/${livyServer.user}/deployments/${trDeployment.name}"

        hdfs.deleteDir(path)
        hdfs.createDir(path)
        def transformationDir = getSourcesDirectoryPath("Transformation") + "/" + trDeployment.transformation.name
        def job = new File("${transformationDir}/target/ru.neoflex.meta.etl2.spark.${trDeployment.transformation.name}-1.0-SNAPSHOT.jar")
        hdfs.putFile("${path}/${job.name}", job)

        LinkedHashMap<String, Object> batchParams = getBatchParams(trDeployment, trDeployment.transformation, params)
        def config = EMF.generate([trDeployment], "/psm/etl/airflow/config.egl", [batchParams: batchParams])
        def configFile = new File(transformationDir, "${trDeployment.transformation.name}.json")
        configFile.text = config
        hdfs.putFile("${path}/${configFile.name}", configFile)
        return [result: true, problems: []]
        /* protected region MetaServer.rtTransformationDeployment.deploy end */
    }


    static isNeedRebuild(Map trDeployment){
        def changeDateTime = getMaxUpdated([trDeployment, trDeployment.transformation])
        def transformation = trDeployment.transformation
        def sourcesDir = getSourcesDirectoryPath("Transformation")
        File statusFile = new File(sourcesDir, "${transformation.name}/status.txt")
        if (changeDateTime == null || !statusFile.isFile() || changeDateTime.time > statusFile.lastModified()){
            return true;
        }
        return false;
    }

    static Object install(Map entity, Map params = null) {
        /* protected region MetaServer.rtTransformationDeployment.install on begin */
        Database db = Database.new
        Map td = db.get(entity)
        def sourcesDir = getSourcesDirectoryPath("Transformation")
        def transformation = td.transformation

        File statusFile = new File(sourcesDir, "${transformation.name}/status.txt")
        if (isNeedRebuild(td)) {
            def result = chain([
                    TransformationDeployment.&generate,
                    TransformationDeployment.&build
            ], entity, params)
            if (result.result) {
                statusFile.write("OK")
            }
            if (!result.result) {
                logger.error(result.problems.toString())
            }
            return result
        }
        return [result: true, problems: []]
        /* protected region MetaServer.rtTransformationDeployment.install end */
    }

    static Object run(Map entity, Map params = null) {
        /* protected region MetaServer.rtTransformationDeployment.run on begin */
        def trDeployment = Database.new.get(entity)
        def transformation = trDeployment.transformation
        def livyServer
        if (params.livyServer != null) livyServer = params.livyServer
        else if (trDeployment.livyServer != null) livyServer = trDeployment.livyServer
        if (trDeployment.master == null || trDeployment.master == '') throw new RuntimeException("master for TransformationDeployment " + trDeployment.name + " is empty")

        LinkedHashMap<String, Object> batchParams = getBatchParams(trDeployment, transformation, params)
        def sessionId
        def result = LivyServer.createBatchSession(livyServer, batchParams).result
        try {
            sessionId = result.id
        } catch (e) {
            throw new RuntimeException(result)
        }
        logger.info("Transformation submitted")
        logger.info("Transformation name:       ${transformation.name}")
        logger.info("Transformation deployment: ${trDeployment.name}")
        logger.info("Livy:                      ${livyServer.name}")
        logger.info("Batch Id:                ${sessionId}")

        return [result: true, problems: [], data: [sessionId: sessionId]]

        /* protected region MetaServer.rtTransformationDeployment.run end */
    }

    static LinkedHashMap<String, Object> getBatchParams(Map trDeployment, transformation, Map params) {
        def livyServer = trDeployment.livyServer
        def path = "${livyServer.home}/${livyServer.user}/deployments/${trDeployment.name}/ru.neoflex.meta.etl2.spark.${transformation.name}-1.0-SNAPSHOT.jar".toString()
        def workflowId = "${transformation.name}_${System.identityHashCode([])}"
        def jobParams = [
                "HOME=${livyServer.home}".toString(),
                "USER=${livyServer.user}".toString(),
                "WF_HOME=${livyServer.home}/${livyServer.user}".toString(),
                "ROOT_WORKFLOW_ID=${workflowId}".toString(),
                "CURRENT_WORKFLOW_ID=${workflowId}".toString(),
                "SLIDE_SIZE=${trDeployment.slideSize}".toString(),
                "FETCH_SIZE=${trDeployment.fetchSize}".toString(),
                "PARTITION_NUM=${trDeployment.partitionNum}".toString(),
                "FAIL_THRESHOLD=${trDeployment.rejectSize}".toString(),
                "DEBUG=${trDeployment.debug}".toString(),
                "MASTER=${trDeployment.master}".toString()
        ]

        trDeployment.deployments.each {
            jobParams.add("JDBC_${it.softwareSystem.name}_URL=${it.connection.url}".toString())
            jobParams.add("JDBC_${it.softwareSystem.name}_USER=${it.connection.user}".toString())
            jobParams.add("JDBC_${it.softwareSystem.name}_DRIVER=${it.connection.driver}".toString())
            jobParams.add("JDBC_${it.softwareSystem.name}_PASSWORD=${SymmetricCipher.encrypt(JdbcConnection.getPassword(it.connection))}".toString())
            jobParams.add("JDBC_${it.softwareSystem.name}_SCHEMA=${it.connection.schema}".toString())
        }
        def parameters = [:]
        trDeployment.parameters.each {
            parameters.put(it.name, it.value)
        }
        parameters.putAll(params)
        for (key in parameters.keySet()) {
            jobParams.add(JSONHelper.escape("${key}=${parameters[key]}").toString())
        }
        def batchParams = [file: path, proxyUser: "${livyServer.user}".toString(), className: "ru.neoflex.meta.etl2.spark.${transformation.name}Job".toString(), args: jobParams]
        if (trDeployment.driverMemory) batchParams["driverMemory"] = trDeployment.driverMemory
        if (trDeployment.executorMemory) batchParams["executorMemory"] = trDeployment.executorMemory
        if (trDeployment.executorCores) batchParams["executorCores"] = trDeployment.executorCores
        if (trDeployment.numExecutors) batchParams["numExecutors"] = trDeployment.numExecutors
        if(trDeployment.sparkConf.size() > 0) {
            def sparkConf = [:]
            for(param in trDeployment.sparkConf) {
                sparkConf.putAt(param.name, param.value)
            }
            batchParams.conf = sparkConf
        }
        batchParams
    }

    static Object runPart(Map entity, Map params = null) {
        /* protected region MetaServer.rtTransformationDeployment.runPart on begin */
        def trDeployment = Database.new.get(entity)
        def livyServer = LivyServer.findCurrentLivyServer(trDeployment, params)
        def sessionId = LivyServer.getSessionId(params, livyServer)
        def result = LivyServer.executeStatementAndWait(sessionId, (String) params.code, logger, livyServer)
        if (result.output.status == 'error') {
            logger.error(result.output.evalue)
            return [result: "text/plain:${result.output}", sessionId: sessionId]
        } else {
            result = result.output.data
            if (params.outputType == 'json') {
                def jsonData = "{fields:[]}"
                if (result != null) {
                    def parsedJson = (result instanceof Map) ? ((result.values()[0] =~ /(?ms).*^(\{.*\})$.*/)) : (result =~ /\{.*\}/)
                    if (parsedJson.matches()) {
                        jsonData = parsedJson.group(1)
                    }
                }
                return [result: jsonData, sessionId: sessionId]
            } else {
                return [result: result, sessionId: sessionId]
            }
        }
        /* protected region MetaServer.rtTransformationDeployment.runPart end */
    }

    static Object generateAndRun(Map entity, Map params = null) {
        /* protected region MetaServer.rtTransformationDeployment.generateAndRun on begin */
        return chain([
                TransformationDeployment.&install,
                TransformationDeployment.&deploy,
                TransformationDeployment.&run,
                TransformationDeployment.&waitJob
        ], entity, params)
        /* protected region MetaServer.rtTransformationDeployment.generateAndRun end */
    }

    static Object buildAndRun(Map entity, Map params = null) {
        /* protected region MetaServer.rtTransformationDeployment.buildAndRun on begin */
        return chain([
                TransformationDeployment.&build,
                TransformationDeployment.&deploy,
                TransformationDeployment.&run,
                TransformationDeployment.&waitJob
        ], entity, params)
        /* protected region MetaServer.rtTransformationDeployment.buildAndRun end */
    }
}
