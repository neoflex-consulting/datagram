package MetaServer.etl


import MetaServer.utils.ECoreHelper
import MetaServer.utils.GenerationBase
import org.eclipse.emf.ecore.EObject
import ru.neoflex.meta.utils.Context
/* protected region MetaServer.etlTransformation.inport on begin */
import org.eclipse.epsilon.common.util.StringProperties
import org.eclipse.epsilon.emc.emf.EmfModel
import ru.neoflex.meta.model.Database
import MetaServer.utils.MetaInfo
import MetaServer.utils.SparkSQL
import MetaServer.rt.TransformationDeployment
import org.apache.commons.logging.Log
import org.apache.commons.logging.LogFactory

import java.nio.file.Files
import java.nio.file.Path
import java.text.SimpleDateFormat
import java.util.concurrent.Callable
import java.util.function.BiConsumer
import java.util.function.BiFunction
import java.util.function.Consumer
import java.util.function.Predicate
import groovy.time.TimeCategory

import static java.nio.file.StandardCopyOption.REPLACE_EXISTING

/* protected region MetaServer.etlTransformation.inport end */
class Transformation extends GenerationBase {
    /* protected region MetaServer.etlTransformation.statics on begin */
    private final static Log logger = LogFactory.getLog(Transformation.class)
    private static SimpleDateFormat jsonTimestampFormatter = new SimpleDateFormat("yyyy-MM-dd'T'HH:mm:ss.SSSZ")

    static List collectTrDeployments(Map tr, List collected) {
        (tr.sources.collect {it} + tr.targets.collect {it}).findAll { it.context != null }.collect {
            def ssname = it.context.name
            def ss = Context.current.session.createQuery("from rt.SoftwareSystem where name = :name").setParameter("name", ssname).uniqueResult()
            if (ss == null) {
                throw new RuntimeException("rt.SoftwareSystem ${ssname} not found")
            }
            if (ss.defaultDeployment == null) {
                throw new RuntimeException("default deployment for rt.SoftwareSystem ${ssname} not defined")
            }
            collected.add(ss.defaultDeployment)
        }
        return collected
    }

    static Map findOrCreateAutogeneratedProject() {
        def name = "autogenerated"
        def db = Database.new
        def project = db.session.createQuery("from etl.Project where name = :name").setParameter("name", name).uniqueResult()
        if (project == null) {
            println("create new project ${name}")
            project = db.instantiate("etl.Project", [name: name])
            db.save(project)
        }
        return project
    }

    static Map findOrCreateWF(name, Map entity) {
        def db = Database.new
        def wf = db.session.createQuery("from etl.Workflow where name = :name").setParameter("name", name).uniqueResult()
        if (wf == null) {
            def project = findOrCreateAutogeneratedProject()
            logger.info("create new workflow ${name}")
            wf = db.instantiate("etl.Workflow", [name: name, project: project])
            def end = db.instantiate("etl.WFEnd", [name: "end"])
            def kill = db.instantiate("etl.WFKill", [name: "kill"])
            def transformation = db.instantiate("etl.WFTransformation", [name: entity.name, transformation: entity, ok: end, error: kill])
            def start = db.instantiate("etl.WFManualStart", [name: "start", to: transformation])
            wf.nodes.addAll([start, transformation, end, kill])
            db.save(wf)
        }
        return wf
    }


    static Object generate(Map entity, Map params, String svnCommitMassage = null) {
        String info = svnCommitMassage == null ? "Generate Transformation sources " + entity.name : svnCommitMassage
        logger.info(info)
        def transformation = new Database("teneo").get("etl.Transformation", (Long) entity.e_id)
        def auditInfo = transformation.auditInfo
        def changeDateTime = auditInfo.changeDateTime
        def gitFlow = Context.current.getContextSvc().getGitflowSvc()
        def sourcesDir = gitFlow.SOURCES + "/Transformation"
        def result = [:]
        return gitFlow.inDir(sourcesDir, info, new BiFunction<Path, Path, Map>() {
            @Override
            Map apply(Path tmp, Path sourcesPath) {
                def pomPath = sourcesPath.resolve("${transformation.name}/pom.xml")
                def transformationDate = gitFlow.getLastModified(pomPath)
                def isRegularFile = Files.isRegularFile(pomPath)
                def changed = changeDateTime != null
                def transformationDateIsNotNull = transformationDate != null
                if (changeDateTime != null) {
                    use (groovy.time.TimeCategory) {
                        changeDateTime = changeDateTime + 3.seconds;
                    }
                }

                if(isRegularFile && changed && transformationDateIsNotNull && transformationDate.after(changeDateTime)) {
                    return [result: true, fileContent: "Don't need to be generated. All is up to date"]
                }
                def emfModel = new EmfModel()
                def properties = new StringProperties()
                properties.put(EmfModel.PROPERTY_NAME, "src")
                properties.put(EmfModel.PROPERTY_MODEL_URI, "hibernate://?dsname=teneo&query1=from etl.Transformation where e_id=${entity.e_id}")
                properties.put(EmfModel.PROPERTY_METAMODEL_URI, "http://www.neoflex.ru/meta/etl")
                properties.put(EmfModel.PROPERTY_READONLOAD, "true")
                emfModel.load(properties, "")

                def transformationGenerator = "/psm/etl/spark/Transformation2.egx";
                if(transformation.sparkVersion == null ||  transformation.sparkVersion.name == "SPARK3"){
                    transformationGenerator = "/psm/etl/spark/Transformation3.egx";
                }
                Context.current.getContextSvc().epsilonSvc.executeEgx(transformationGenerator, [mspaceRoot: tmp.toUri().toString(),  nodeName: params.nodeName, outputType: params.outputType], [emfModel])
                def tests = Database.new.session.createQuery("from etl.TransformationTest where transformation.e_id=${entity.e_id}").list()
                for(t in tests) {
                    if(!t.enabled){
                        continue
                    }
                    EmfModel testModel = new EmfModel()
                    StringProperties testProperties = new StringProperties()
                    testProperties.put(EmfModel.PROPERTY_NAME, "src")
                    testProperties.put(EmfModel.PROPERTY_MODEL_URI, "hibernate://?dsname=teneo&query1=from etl.TransformationTest where e_id=${t.e_id}")
                    testProperties.put(EmfModel.PROPERTY_METAMODEL_URI, "http://www.neoflex.ru/meta/etl")
                    testProperties.put(EmfModel.PROPERTY_READONLOAD, "true")
                    testModel.load(testProperties, "")
                    Context.current.getContextSvc().epsilonSvc.executeEgx("/psm/etl/spark/TransformationTest.egx", [mspaceRoot: tmp.toUri().toString(), nodeName: params.nodeName, outputType: params.outputType], [testModel])
                }

                gitFlow.copyContentRecursive(tmp.resolve(transformation.name), sourcesPath.resolve(transformation.name), true)
                return [result: true, fileContent: "", problems: ""]
            }
        })
    }

    static Object build(Map entity, Map params){
        logger.info("Run Transformation build")
        def database = new Database("teneo")
        def transformation = database.get("etl.Transformation", (Long) entity.e_id)
        def tests = database.session.createQuery("from etl.TransformationTest where transformation.e_id=${entity.e_id}").list()
        def enabledTestsList = new ArrayList()
        def packagePrefix = "ru.neoflex.meta.etl2.spark"
        for(t in tests){
            if(t.enabled){
                enabledTestsList.add(packagePrefix + "." + transformation.name + t.name + "Test")
            }
        }
        def haveEnabledTests = enabledTestsList.size() > 0

        Map map = new HashMap()
        if(haveEnabledTests){
            def testInclString = "" + enabledTestsList.join(",")
            map.put("suites", testInclString)
        }else{
            map.put("maven.test.skip", "true")
        }
        def gitFlow = Context.current.getContextSvc().getGitflowSvc()
        def transformationDir = gitFlow.SOURCES + "/Transformation/" + transformation.get("name")
        return gitFlow.inDir(transformationDir, "Run Transformation build", new BiFunction<Path, Path, Map>() {
            @Override
            Map apply(Path tmp, Path transformationPath) {
                gitFlow.copyContentRecursive(transformationPath, tmp)
                def pomPath = tmp.resolve("pom.xml")
                Context.current.getContextSvc().getMavenSvc().run(pomPath.toFile(), "clean,install", null, null, null, map)
                //PathMatcher filter = tmp.getFileSystem().getPathMatcher("glob:*.jar")
                Files.walk(tmp).filter(Files.&isRegularFile).forEach(new Consumer<Path>() {
                    @Override
                    void accept(Path source) {
                        try {
                            if (source.toString().toLowerCase().endsWith(".jar")) {
                                Path dstFile = transformationPath.resolve(tmp.relativize(source).toString().replace('\\', '/'))
                                Files.createDirectories(dstFile.getParent())
                                Files.copy(source, dstFile, REPLACE_EXISTING)
                            }
                        } catch (IOException e) {
                            throw new RuntimeException(e.getMessage(), e)
                        }
                    }
                })
                return [result: true, fileContent: "", problems: ""]
            }
        })
    }

    static Object generateTests(Map entity, Map params) {
        def db = Database.new
        def transformation = db.get("etl.Transformation", (Long) entity.e_id)
        def tests = db.session.createQuery("from etl.TransformationTest where transformation.e_id=${entity.e_id}").list()
        def gitFlow = Context.current.getContextSvc().getGitflowSvc()
        return gitFlow.inDir(gitFlow.SOURCES + "/Transformation", "generate tests", new BiFunction<Path, Path, Map>() {
            @Override
            Map apply(Path tmp, Path sourcesPath) {
                for(t in tests) {
                    if(!t.enabled){
                        continue
                    }
                    EmfModel testModel = new EmfModel()
                    StringProperties testProperties = new StringProperties()
                    testProperties.put(EmfModel.PROPERTY_NAME, "src")
                    testProperties.put(EmfModel.PROPERTY_MODEL_URI, "hibernate://?dsname=teneo&query1=from etl.TransformationTest where e_id=${t.e_id}")
                    testProperties.put(EmfModel.PROPERTY_METAMODEL_URI, "http://www.neoflex.ru/meta/etl")
                    testProperties.put(EmfModel.PROPERTY_READONLOAD, "true")
                    testModel.load(testProperties, "")
                    Context.current.getContextSvc().epsilonSvc.executeEgx("/psm/etl/spark/TransformationTest.egx", [mspaceRoot: tmp.toUri().toString(), nodeName: params.nodeName, outputType: params.outputType], [testModel])
                }
                def source = tmp.resolve(transformation.name)
                if (Files.isDirectory(source)) {
                    gitFlow.copyContentRecursive(source, sourcesPath.resolve(transformation.name), false)
                }
                return [result: true, problems: []]
            }
        })
    }

    static Object checkTRD(Map transformation, Map params = null) {
        def db = Database.new
        def tr = db.get(transformation)
        def trdList = db.session.createQuery("from rt.TransformationDeployment where transformation.e_id=${tr.e_id}").list()
        if (trdList.size() == 0) {
            def trd = createTRD("autogenerated_tr_" + tr.name, tr)
        }
        return [result: true]
    }

    static Map findOrCreateTRD(Map transformation) {
        return findOrCreateTRD("autogenerated_tr_" + transformation.name, transformation)
    }

    static Map findOrCreateTRD(name, Map transformation) {
        def db = Database.new
        def trdList
        trdList = db.session.createQuery("from rt.TransformationDeployment where transformation.e_id=${transformation.e_id}").list()
        if (trdList.size() == 1) {return adjustTRD(trdList.get(0))}
        if (trdList.size() > 1) {
            def defaultTRDs = trdList.findAll {it.isDefault == true}
            if (defaultTRDs.size() > 1) throw new RuntimeException("Multiple default TransformationDeployments for Transformation ${transformation.name} found")
            if (defaultTRDs.size() == 0) throw new RuntimeException("Multiple TransformationDeployments for Transformation ${transformation.name} found. Set default TransformationDeployment")
            return defaultTRDs.get(0)
        }
        if (trdList.size() == 0) {
           return createTRD(name, transformation)
        }
    }

    static adjustTRD(Map trd) {
        def deployments = []
        def count = 0
        collectTrDeployments(trd.transformation, deployments)
        deployments.each { d->
            if (!trd.deployments.any {it.name == d.name}) {
                trd.deployments.add(d)
                count += 1
            }
        }
        if (count > 0) {
            Database.new.save(trd)
        }
        return trd
    }

    static Map createTRD(name, Map transformation) {
        logger.info("create new TransformationDeployment ${name}")
        def project = transformation.project
        def livyServer = null
        if (project != null) {
            livyServer = Context.current.session.createQuery("from rt.LivyServer where project.e_id=${project.e_id}").uniqueResult()
        }
        if (livyServer == null) {
            livyServer = Context.current.session.createQuery("from rt.LivyServer where isDefault=true").uniqueResult()
        }
        if (livyServer == null)  {
            throw new RuntimeException("Default LivyServer and LivyServer for project ${project?.name} not found")
        }
        if (project == null) {
            project = Project.findOrCreateProject()
        }
        def deployments = []
        collectTrDeployments(transformation, deployments)
        def db = Database.new
        def props = [name: name, project: project, livyServer: livyServer, transformation: transformation,
                     deployments: deployments.unique {"${it._type_}|${it.e_id}"}, debug: true, slideSize: 400,
                     rejectSize: 1000, fetchSize: 1000, partitionNum: 1, persistOnDisk: true,
                     master: "yarn", mode: "cluster", isDefault: true,
                     driverMemory: livyServer.driverMemory, executorMemory: livyServer.executorMemory,
                     executorCores: livyServer.executorCores, numExecutors: livyServer.numExecutors]
        def trd = db.instantiate("rt.TransformationDeployment", props)
        db.save(trd)
        return trd
    }

    static Object validateModel(Map entity) {
        def fileName = "/pim/etl/etl.evl"
        def emfModel = new EmfModel()
        def properties = new StringProperties()
        properties.put(EmfModel.PROPERTY_NAME, "src")
        properties.put(EmfModel.PROPERTY_MODEL_URI, "hibernate://?dsname=teneo&query1=from etl.Transformation where e_id=${entity.e_id}")
        properties.put(EmfModel.PROPERTY_METAMODEL_URI, "http://www.neoflex.ru/meta/etl")
        properties.put(EmfModel.PROPERTY_READONLOAD, "true")
        emfModel.load(properties, "" )
        def problems = []
        Context.current.getContextSvc().epsilonSvc.executeEvl(fileName, [:], [emfModel], problems)
        return [result: !problems.any{it.isCritique == false}, problems: problems]
    }

    static Object validateScripts(Map entity) {
        def loaded = new Database("teneo").get("etl.Transformation", (Long)entity.e_id)
        Database.new.refresh(loaded)
        loaded.values()
        def problems = []
        validateProjections(loaded, problems)
        validateSelections(loaded, problems)
        validateJoins(loaded, problems)
        validateExpressionSources(loaded, problems)
        validateAggregation(loaded, problems)
        validateOutputPorts(loaded, problems)
        return [result: (problems.find {it.isCritique == false} == null), problems: problems]
    }

    private static boolean validateProjections(Map entity, List problems) {
        int errors = 0
        for (step in entity.transformationSteps) {
            if (step._type_ == "etl.Projection") {
                for (field in step.outputPort.fields) {
                    if ("TRANSFORM" == field.fieldOperationType.toString()) {
                        //def testResult = [result: true]
                        def testResult = ProjectionField.test(field)
                        if (testResult.result != true) {
                            problems.add( [
                                    message: testResult.message,
                                    isCritique: false,
                                    context: "ProjectionField.${step.name}.${field.name}".toString(),
                                    constraint: "CheckScript",
                                    _type_: step._type_,
                                    e_id: step.e_id
                            ])
                            errors += 1
                        }
                    }
                }
            }
        }
        return errors > 0
    }

    private static boolean validateOutputPorts(Map entity, List problems) {
        int errors = 0
        def steps = entity.transformationSteps.collect {it} + entity.sources.collect {it}
        for (step in steps) {
            for (debug in step.outputPort.debugList) {
                def testResult = DebugOutput.test(debug)
                if (testResult.result != true) {
                    problems.add( [
                            message: testResult.message,
                            isCritique: false,
                            context: "DebugOutput(${entity.name}.${step.name}.${debug.name})".toString(),
                            constraint: "CheckScript",
                            _type_: step._type_,
                            e_id: step.e_id
                    ])
                    errors += 1
                }
            }
        }
        return errors > 0
    }

    private static boolean validateJoins(Map entity, List problems) {
        int errors = 0
        for (step in entity.transformationSteps) {
            if (step._type_ == "etl.Join") {
                for (field in step.outputPort.fields) {
                    if ("TRANSFORM" == field.fieldOperationType.toString()) {
                        def testResult = ProjectionField.test(field)
                        if (testResult.result != true) {
                            problems.add( [
                                    message: testResult.message,
                                    isCritique: false,
                                    context: "ProjectionField.${entity.name}.${field.name}".toString(),
                                    constraint: "CheckScript",
                                    _type_: step._type_,
                                    e_id: step.e_id
                            ])
                            errors += 1
                        }
                    }
                }
            }
        }
        return errors > 0
    }

    private static boolean validateSelections(Map entity, List problems) {
        int errors = 0
        for (step in entity.transformationSteps) {
            if (step._type_ == "etl.Selection") {
                def testResult = Selection.test(step)
                if (testResult.result != true) {
                    problems.add( [
                            message: testResult.message,
                            isCritique: false,
                            context: "Selection.${entity.name}".toString(),
                            constraint: "CheckScript",
                            _type_: step._type_,
                            e_id: step.e_id
                    ])
                    errors += 1
                }
            }
        }
        return errors > 0
    }

    private static boolean validateExpressionSources(Map entity, List problems) {
        int errors = 0
        for (step in entity.transformationSteps) {
            if (step._type_ == "etl.ExpressionSource") {
                def testResult = ExpressionSource.test(step)
                if (testResult.result != true) {
                    problems.add( [
                            message: testResult.message,
                            isCritique: false,
                            context: "ExpressionSource.${entity.name}".toString(),
                            constraint: "CheckScript",
                            _type_: step._type_,
                            e_id: step.e_id
                    ])
                    errors += 1
                }
            }
        }
        return errors > 0
    }

    private static boolean validateAggregation(Map entity, List problems) {
        int errors = 0
        for (step in entity.transformationSteps) {
            if (step._type_ == "etl.Aggregation" && step.aggregationFunction.toString() == "USER_DEF") {
                def testResult = Aggregation.testExpression(step)
                if (testResult.result != true) {
                    problems.add( [
                            message: testResult.message,
                            isCritique: false,
                            context: "ExpressionSource.${entity.name}".toString(),
                            constraint: "CheckExpression",
                            _type_: step._type_,
                            e_id: step.e_id
                    ])
                    errors += 1
                }
                testResult = Aggregation.testInitExpression(step)
                if (testResult.result != true) {
                    problems.add( [
                            message: testResult.message,
                            isCritique: false,
                            context: "ExpressionSource.${entity.name}".toString(),
                            constraint: "CheckInitExpression",
                            _type_: step._type_,
                            e_id: step.e_id
                    ])
                    errors += 1
                }
                testResult = Aggregation.testFinalExpression(step)
                if (testResult.result != true) {
                    problems.add( [
                            message: testResult.message,
                            isCritique: false,
                            context: "ExpressionSource.${entity.name}".toString(),
                            constraint: "CheckFinalExpression",
                            _type_: step._type_,
                            e_id: step.e_id
                    ])
                    errors += 1
                }
            }
        }
        return errors > 0
    }


    static Object validateTests(Map transformation, Map params =null) {
        def fileName = "/pim/etl/etl.evl"

        def db = Database.new
        def testList
        testList = db.session.createQuery("from etl.TransformationTest where transformation.e_id=${transformation.e_id}").list()
        def problems = []
        for(t in testList){
            def emfModel = new EmfModel()
            def properties = new StringProperties()
            properties.put(EmfModel.PROPERTY_NAME, "src")
            properties.put(EmfModel.PROPERTY_MODEL_URI, "hibernate://?dsname=teneo&query1=from etl.TransformationTest where e_id=${t.e_id}")
            properties.put(EmfModel.PROPERTY_METAMODEL_URI, "http://www.neoflex.ru/meta/etl")
            properties.put(EmfModel.PROPERTY_READONLOAD, "true")
            emfModel.load(properties, "" )
            Context.current.getContextSvc().epsilonSvc.executeEvl(fileName, [:], [emfModel], problems)
        }
        return [result: !problems.any{it.isCritique == false}, problems: problems]
    }

    private static Map inputFieldDependencies(Map transformation, Map field) {
        def outputPort = findOutputPortByInputPort(transformation, field.dataSet)
        def innode = findNodeByOutputPort(transformation, outputPort)
        fieldDependencies(transformation, innode, innode.outputPort.fields.find {it.name == field.name})
    }

    private static List getInputPorts(Map node) {
        def result = []
        if (node.inputPort != null) result.add(node.inputPort)
        if (node.sqlPorts != null) result.addAll(node.sqlPorts)
        if (node.joineePort != null) result.add(node.joineePort)
        if (node.unionPort != null) result.add(node.unionPort)
        return result
    }

    private static Map outputFieldFollowers(Map transformation, Map node, Map field) {
        def result = [fieldName: field.name, fieldType: field.dataTypeDomain.toString() , nodeName: node.name, followers: []]
        findInputPortsByOutputPort(transformation, field.dataSet).each {
            def outnode = findNodeByInputPort(transformation, it)
            def planDep = null
            if (outnode._type_ == "etl.SparkSQL") {
                try {
                    planDep = SparkSQL.extractDepsFromSQL(outnode.statement)
                }
                catch (th) {
                    println("error parsing SparkSQL: " + th.toString())
                }
            }
            def infield = it.fields.find {it.name == field.name}
            if (outnode.outputPort != null) {
                if (planDep != null) {
                    result.followers.addAll(outnode.outputPort.fields.findAll {outField->
                        (planDep[outField.name] ?: []).any {
                            it.name.equalsIgnoreCase(infield.name) && it.alias.equalsIgnoreCase(infield.dataSet.alias)
                        } || (planDep["*"] ?: []).any {
                            outField.name.equalsIgnoreCase(infield.name) && it.alias.equalsIgnoreCase(infield.dataSet.alias)
                        }
                    }.collect {
                        outputFieldFollowers(transformation, outnode, it)
                    })
                }
                else {
                    result.followers.addAll(outnode.outputPort.fields.findAll {it._type_ == 'dataset.Field' && it.name == infield.name}.collect {
                        outputFieldFollowers(transformation, outnode, it)
                    })
                    result.followers.addAll(outnode.outputPort.fields.findAll {it._type_ == 'etl.ProjectionField' && it.sourceFields.any {it.name == infield.name}}.collect {
                        outputFieldFollowers(transformation, outnode, it)
                    })
                    result.followers.addAll(outnode.outputPort.fields.findAll {it._type_ == 'etl.UnionField' && it.inputPortField?.e_id == infield.e_id}.collect {
                        outputFieldFollowers(transformation, outnode, it)
                    })
                    result.followers.addAll(outnode.outputPort.fields.findAll {it._type_ == 'etl.UnionField' && it.unionPortField?.e_id == infield.e_id}.collect {
                        outputFieldFollowers(transformation, outnode, it)
                    })
                }
            }
            else {
                result.followers.add([fieldName: infield.name, fieldType: infield.dataTypeDomain.toString() , nodeName: outnode.name, followers: []])
            }
        }
        return result
    }

    private static Map fieldDependencies(Map transformation, Map node, Map field) {
        def result = [fieldName: field.name, fieldType:field.dataTypeDomain.toString() , nodeName: node.name, dependencies: []]
        if (field._type_ == "dataset.Field") {
            if (node.inputPort != null) {
                def inputFields = node.inputPort.fields.findAll {it.name == field.name}
                result.dependencies.addAll(inputFields.collect {
                    inputFieldDependencies(transformation, it)
                })
            }
            if (node.sqlPorts != null) {
                try {
                    def planDeps = SparkSQL.extractDepsFromSQL(node.statement)
                    def deps = planDeps[field.name] ?: []
                    def starDeps = planDeps["*"]
                    if (starDeps != null) {
                        deps.addAll(starDeps.collect {[alias: it.alias, name: field.name]})
                    }
                    deps.each {dep->
                        node.sqlPorts.findAll {sqlPort->
                            sqlPort.alias.equalsIgnoreCase(dep.alias)
                        }.each {sqlPort->
                            result.dependencies.addAll(sqlPort.fields.findAll {it.name.equalsIgnoreCase(dep.name)}.collect {
                                inputFieldDependencies(transformation, it)
                            })
                        }

                    }
                }
                catch (th) {
                    println("error parsing sql - compare field names")
                    for (sqlPort in node.sqlPorts) {
                        def inputFields = sqlPort.fields.findAll {it.name == field.name}
                        result.dependencies.addAll(inputFields.collect {
                            inputFieldDependencies(transformation, it)
                        })
                    }
                }
            }
        }
        else if (field._type_ == "etl.ProjectionField") {
            result.dependencies.addAll(field.sourceFields.collect {
                inputFieldDependencies(transformation, it)
            })
        }
        else if (field._type_ == "etl.UnionField") {
            if (field.inputPortField != null) {
                result.dependencies.add(inputFieldDependencies(transformation, field.inputPortField))
            }
            if (field.unionPortField != null) {
                result.dependencies.add(inputFieldDependencies(transformation, field.unionPortField))
            }
        }
        return result
    }

    private static Map findNodeByOutputPort(Map transformation, Map outputPort) {
        return ([] + transformation.sources + transformation.transformationSteps).find {it.outputPort.e_id == outputPort.e_id}
    }

    private static Map findNodeByInputPort(Map transformation, Map inputPort) {
        return ([] + transformation.transformationSteps + transformation.targets).find {getInputPorts(it).contains(inputPort)}
    }

    private static Map findOutputPortByInputPort(Map transformation, Map inputPort) {
        return transformation.transitions.findAll {it.finish.e_id == inputPort.e_id}.collect{it.start}.first()
    }

    private static List findInputPortsByOutputPort(Map transformation, Map outputPort) {
        return transformation.transitions.findAll {it.start.e_id == outputPort.e_id}.collect{it.finish}
    }

    static Object expandXMLFile(Map entity, Map params = null) {
      def result = []
      def tr = Database.new.get(entity)
      def trd = findOrCreateTRD(tr)
      Context.current.commit()

      def code = Context.current.getContextSvc().epsilonSvc.executeEgl("/psm/etl/spark/SourceStructureExpand.egl", [format: "xml", fileName: entity.fileName, rowTag: entity.rowTag, sampleSize: entity.sampleSize, dontExplode: entity.dontExplode], [])
      return TransformationDeployment.runPart(trd, [code: code, outputType: 'json', sessionId: entity.sessionId])
    }

    static Object expandAvroFile(Map entity, Map params = null) {
      def result = []
      def tr = Database.new.get(entity)
      def trd = findOrCreateTRD(tr)
      Context.current.commit()

      def code = Context.current.getContextSvc().epsilonSvc.executeEgl("/psm/etl/spark/SourceStructureExpand.egl", [format: "avro", fileName: entity.fileName, rowTag: entity.rowTag, sampleSize: entity.sampleSize, dontExplode: entity.dontExplode], [])
      return TransformationDeployment.runPart(trd, [code: code, outputType: 'json', sessionId: entity.sessionId])
    }

    static Object importTransformation(Map entity, Map params = null) {
        def transformation = Database.new.get(entity)
        def project = transformation.project
        if (project != null && project.svnEnabled) {
            MetaServer.etl.Project.importProjectEntity(project, transformation)
        }else{
            return Context.current.getContextSvc().getGitflowSvc().importEntityByName(entity)
        }
    }


    static Object exportTransformation(Map entity, Map params = null) {
        def transformation = Database.new.get(entity)
        def project = transformation.project
        def svnCommitMessage = params.svnCommitMessage
        if (project != null) {
            MetaServer.etl.Project.exportProjectEntity(project, transformation, svnCommitMessage)
        }
        return exportProjectEntityWithDependentObjects(transformation, params, svnCommitMessage)
    }

    static List exportProjectEntityWithDependentObjects(Map entity, Map params, String svnCommitMessage) {
        def eCoreHelper = new ECoreHelper()
        def files = new ArrayList<File>()
        def filesDeps = eCoreHelper.getAllDependentObjectsOfEntity(entity)
        def gitFlow = Context.current.getContextSvc().getGitflowSvc()
        for(Object e : filesDeps){
            def ent= Database.new.get(e)
            List<File> res  = gitFlow.exportEObject((EObject)ent, svnCommitMessage)
            files.addAll(res)
        }
        generate(entity, params, svnCommitMessage)
        return files
    }
    /* protected region MetaServer.etlTransformation.statics end */

    static Object validate(Map entity, Map params = null) {
    /* protected region MetaServer.etlTransformation.validate on begin */
        def retModel = validateModel(entity)
        def retScripts = validateScripts(entity)
        def ret = [result: retModel.result && retScripts.result, problems: retModel.problems + retScripts.problems]
        for (problem in ret.problems) {
            logger.warn(problem)
        }
        return ret
    /* protected region MetaServer.etlTransformation.validate end */
    }

    static Object dependencies(Map entity, Map params = null) {
    /* protected region MetaServer.etlTransformation.dependencies on begin */
        def deps = MetaInfo.getAllDeps(entity).collect {[_type_: it._type_, name: it.name, e_id: it.e_id]}
        println(deps)
        return deps
    /* protected region MetaServer.etlTransformation.dependencies end */
    }

    static Object setJsonView(Map entity, Map params = null) {
    /* protected region MetaServer.etlTransformation.setJsonView on begin */
        Project.setEntityJsonView(entity._type_, entity.e_id, true)
        return ["Hello from MetaServer.etl.Transformation.setJsonView"]
    /* protected region MetaServer.etlTransformation.setJsonView end */
    }

    static Object install(Map entity, Map params = null) {
    /* protected region MetaServer.etlTransformation.install on begin */
        def tr = Database.new.get(entity)
        def trd = findOrCreateTRD(tr)
        Context.current.commit()
        return TransformationDeployment.install(trd, params)
    /* protected region MetaServer.etlTransformation.install end */
    }

    static Object runit(Map entity, Map params = null) {
    /* protected region MetaServer.etlTransformation.runit on begin */
        def trd = entity.trd ? Database.new.get(entity.trd) : findOrCreateTRD(Database.new.get(entity))
        Context.current.commit()
        def allParams = entity.params ?: [:] as Map
        allParams.putAll(params ?: [:])
        return TransformationDeployment.generateAndRun(trd, allParams)
    /* protected region MetaServer.etlTransformation.runit end */
    }

    static Object runPart(Map entity, Map params = null) {
    /* protected region MetaServer.etlTransformation.runit on begin */
        def tr = Database.new.get(entity)
        def trd = findOrCreateTRD(tr)
        Context.current.commit()
        def outputType
        if (entity.outputType == null) outputType = 'text' else outputType = entity.outputType
        return TransformationDeployment.runPart(trd, [livyServer: entity.server, code: entity.fileContent, sessionId: entity.sessionId, outputType: outputType])
    /* protected region MetaServer.etlTransformation.runit end */
    }

    static Object partJobFile(Map entity, Map params = null) {
    /* protected region MetaServer.etlTransformation.runit on begin */
      def result = []
      def tr = Database.new.get(entity)
      def trd = findOrCreateTRD(tr)
      Context.current.commit()
      def outputType
      if (entity.outputType == null) outputType = 'text' else outputType = entity.outputType
      return TransformationDeployment.generatePart(trd, [nodeName: entity.nodeName, outputType: outputType, sampleSize: entity.sampleSize])
    /* protected region MetaServer.etlTransformation.runit end */
    }

    static Object generateAndRunPart(Map entity, Map params = null) {
    /* protected region MetaServer.etlTransformation.runit on begin */
      def result = []
      def tr = Database.new.get(entity)
      def trd = findOrCreateTRD(tr)
      Context.current.commit()
      def outputType
      if (entity.outputType == null) outputType = 'text' else outputType = entity.outputType
      def fileContent = TransformationDeployment.generatePart(trd, [nodeName: entity.nodeName, outputType: outputType, sampleSize: entity.sampleSize, statement: entity.statement]).fileContent
      return TransformationDeployment.runPart(trd, [code: fileContent, outputType: outputType, sessionId: entity.sessionId])
    /* protected region MetaServer.etlTransformation.runit end */
    }

    static Object mainJobFile(Map entity, Map params = null) {
    /* protected region MetaServer.etlTransformation.runit on begin */
    	def result = []
      def tr = Database.new.get(entity)
      def trd = findOrCreateTRD(tr)
      Context.current.commit()
      return TransformationDeployment.readFile(trd, params)
    /* protected region MetaServer.etlTransformation.runit end */
    }

    static Object writeMainJobFile(Map entity, Map params = null) {
    /* protected region MetaServer.etlTransformation.runit on begin */
    	def result = []

      def tr = Database.new.get(entity)
      def trd = findOrCreateTRD(tr)
      Context.current.commit()
      return TransformationDeployment.writeFile(trd, params, entity.fileContent)
    /* protected region MetaServer.etlTransformation.runit end */
    }

    static Object nodeDependencies(Map entity, Map params = null) {
    /* protected region MetaServer.etlTransformation.nodeDependencies on begin */
        def transformation_Id = entity?.transformation_Id ?: entity?.e_id ?: params?.transformation_Id
        def node_name = entity?.node_name ?: params?.node_name
        def transformation = Database.new.get([_type_:"etl.Transformation", e_id: transformation_Id])
        def dependencies = []
        def followers = []
        def node = ([] + transformation.sources + transformation.transformationSteps).find {it.name == node_name}
        if (node == null) {
            node = transformation.targets.find {it.name == node_name}
            if (node == null) {
                throw new RuntimeException("${node_name} not found")
            }
            dependencies = node.inputPort.fields.collect{fieldDependencies(transformation, node, it)}
            followers = node.inputPort.fields.collect{outputFieldFollowers(transformation, node, it)}
        }
        else {
            dependencies = node.outputPort.fields.collect{fieldDependencies(transformation, node, it)}
            followers = node.outputPort.fields.collect{outputFieldFollowers(transformation, node, it)}
        }
        SparkSQL.clearPlanCache()
        return [dependencies: dependencies, followers: followers]
    /* protected region MetaServer.etlTransformation.nodeDependencies end */
    }

    static Object svnProps(Map entity, Map params = null) {
        def transformation = Database.new.get(entity)
        def project = transformation.project
        return Project.svnProps(project, transformation)
    }
}
