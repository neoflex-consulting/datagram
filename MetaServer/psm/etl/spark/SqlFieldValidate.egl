[%
import "Utils.egl";
%]
import spark.implicits._
import org.apache.spark.sql._
import org.apache.spark.sql.functions._
import scala.collection.mutable
[%
var fields1 = Sequence{};
var fields2 = Sequence{};
var fakeField = Map {"name" = "fakeFieldIDDQD", "javaDomain" = "java.lang.Integer"};
fields1.add(fakeField);
fields2.add(fakeField);

for (f in field.get('sourceFields')) {
	if(f.get("sourceType") == "in") {
		fields1.add(f);
	} else {
		fields2.add(f);
	}
}
 
%]
val jobParameters: mutable.HashMap[String, AnyRef] = new mutable.HashMap[String, AnyRef]()
[%for (param in parameters) {%]
jobParameters.put("[%=param.get("name")%]", s"""[%=param.get("value").replace("\\\\/", "/")%]""")
[%}%]

[%if(field.get("parentType") == "devs.transformation.Projection") { %]
val df = Seq(([%for (f in fields1) {%]null.asInstanceOf[[%=f.get('javaDomain')%]][%if (hasMore){%],[%}%][%}%])).toDF([%for (f in fields1) {%]"[%=f.get('name')%]"[%if (hasMore){%],[%}%][%}%])
[%} else {%]
val df1 = Seq(([%for (f in fields1) {%]null.asInstanceOf[[%=f.get('javaDomain')%]][%if (hasMore){%],[%}%][%}%])).toDF([%for (f in fields1) {%]"[%=f.get('name')%]"[%if (hasMore){%],[%}%][%}%])
val df2 = Seq(([%for (f in fields2) {%]null.asInstanceOf[[%=f.get('javaDomain')%]][%if (hasMore){%],[%}%][%}%])).toDF([%for (f in fields2) {%]"[%=f.get('name')%]"[%if (hasMore){%],[%}%][%}%]) 
val df = df1.joinWith(df2, df1("fakeFieldIDDQD") === df2("fakeFieldIDDQD"))
[%}%]

val ftype = df.select(expr(s"""[%=interpolareParameters(field.get("expression"))%]""").alias("[%=field.get('name')%]")).schema("[%=field.get('name')%]").dataType.typeName

assert(ftype.startsWith("[%=field.get('dataTypeDomain')%]".toLowerCase))