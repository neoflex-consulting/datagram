[%
import "Utils.egl";
%]
case class SelectionClass(
        [%for (field in step.get('inputPort').get('fields')) {%]
        [%=field.get("name")%]: [%=javaTypeName(field.get("dataTypeDomain"))%][%if (hasMore){%],[%}%]
        
        [%}%]
)
object SelectionValidate extends Serializable {
    import org.apache.spark
    import org.apache.spark.sql._
    import org.apache.spark.sql.functions._
    import scala.collection.mutable

    val jobParameters: mutable.HashMap[String, AnyRef] = new mutable.HashMap[String, AnyRef]()
	def run(spark: SparkSession) = {
	  	import spark.implicits._

[%for (param in parameters) {%]
    	jobParameters.put("[%=param.get("name")%]", s"""[%=param.get("value").replace("\\\\/", "/")%]""")
[%}%]
        val df = Seq(SelectionClass(
        [%for (field in step.get('inputPort').get('fields')) {%]
            [%=field.get("name")%] = null.asInstanceOf[ [%=javaTypeName(field.get("dataTypeDomain"))%] ][%if (hasMore){%],[%}%]
        
        [%}%]
        )).toDF([%for (f in step.get('inputPort').get('fields')) {%]"[%=f.get('name')%]"[%if (hasMore){%],[%}%]
    [%}%])

        val ftype = df.select(expr(s"""[%=interpolareParameters(step.get('expression'))%]""")).schema(0).dataType.typeName
        assert(ftype.startsWith("bool"))
    }
}

SelectionValidate.run(spark)
